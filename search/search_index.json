{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"How do I science? Welcome to the Palomero Lab How-To guide! This guide covers a range of topics including: Running bioinformatics tools and pipelines Managing data with command-line interfaces Writing effective scripts for automation Setting up and using AWS services for high-performance computing Using Rstudio and Jupyter notebooks for data analysis Read more about this guide or visit the project's original GitHub repository . Note Lab members click here to access the private section of the guide. How to use this guide Browse the navigation bar, use the search function, or click through the pages to find the information you need. Shell commands and code snippets will be formatted like this when written inline or in a code block like the one below. # Click the clipboard icon to copy me! echo \"Hello, world!\" Important If you get this following error while using zsh : $ zsh: command not found: # Then you need to run the following command to tell zsh to ignore comments: setopt interactivecomments To make this change permanent, add the command to your .zshrc file by either editing it with a text editor or running the following command: echo \"setopt interactivecomments\" >> ~/.zshrc Contributing This guide is open-source and welcomes contributions. If you have a suggestion, correction, or new content to add, you can click the \"Edit this page\" link at the top of each page Happy computing!","title":"How do I science?"},{"location":"#how-do-i-science","text":"Welcome to the Palomero Lab How-To guide! This guide covers a range of topics including: Running bioinformatics tools and pipelines Managing data with command-line interfaces Writing effective scripts for automation Setting up and using AWS services for high-performance computing Using Rstudio and Jupyter notebooks for data analysis Read more about this guide or visit the project's original GitHub repository . Note Lab members click here to access the private section of the guide.","title":"How do I science?"},{"location":"#how-to-use-this-guide","text":"Browse the navigation bar, use the search function, or click through the pages to find the information you need. Shell commands and code snippets will be formatted like this when written inline or in a code block like the one below. # Click the clipboard icon to copy me! echo \"Hello, world!\" Important If you get this following error while using zsh : $ zsh: command not found: # Then you need to run the following command to tell zsh to ignore comments: setopt interactivecomments To make this change permanent, add the command to your .zshrc file by either editing it with a text editor or running the following command: echo \"setopt interactivecomments\" >> ~/.zshrc","title":"How to use this guide"},{"location":"#contributing","text":"This guide is open-source and welcomes contributions. If you have a suggestion, correction, or new content to add, you can click the \"Edit this page\" link at the top of each page Happy computing!","title":"Contributing"},{"location":"about/","text":"About this guide Foreword I was inspired by Tony Narlock's The Tao of tmux by take the collection of notes I had written on Linux-based bioinformatic workflows and turn them into a guide for others to use. After all, science is a collaborative effort and what good does documentation do if it's not FAIR (Findable, Accessible, Interoperable, Reusable) ? Document structure These files are written using GitHub Flavored Markdown (GFM) , a superset of the original lightweight markup language with plain text formatting syntax. Web This guide is built using MkDocs and hosted on GitHub Pages using the Read the Docs theme with some additional configuration.","title":"About this guide"},{"location":"about/#about-this-guide","text":"","title":"About this guide"},{"location":"about/#foreword","text":"I was inspired by Tony Narlock's The Tao of tmux by take the collection of notes I had written on Linux-based bioinformatic workflows and turn them into a guide for others to use. After all, science is a collaborative effort and what good does documentation do if it's not FAIR (Findable, Accessible, Interoperable, Reusable) ?","title":"Foreword"},{"location":"about/#document-structure","text":"These files are written using GitHub Flavored Markdown (GFM) , a superset of the original lightweight markup language with plain text formatting syntax.","title":"Document structure"},{"location":"about/#web","text":"This guide is built using MkDocs and hosted on GitHub Pages using the Read the Docs theme with some additional configuration.","title":"Web"},{"location":"amazon-web-services/","text":"Amazon Web Services This guide covers setting up and using the AWS Command Line Interface (CLI) for interacting with various AWS services. S3 (Simple Storage Service) EC2 (Elastic Compute Cloud) IAM (Identity and Access Management) AWS CLI Installation For fresh Ubuntu instances: curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip && sudo ./aws/install && rm -rf ./awscliv2.zip For other operating systems, refer to the official installation guide . Shell Completion Setup Set up shell completion for the AWS CLI so you can use tab to autocomplete commands and options. Bash Add to your ~/.bashrc : complete -C '/usr/local/bin/aws_completer' aws Zsh Add to your ~/.zshrc : autoload bashcompinit && bashcompinit autoload -Uz compinit && compinit complete -C '/usr/local/bin/aws_completer' aws Important If you installed the AWS CLI in a different location, update the path accordingly. For example, if you installed it using homebrew, the path might be /opt/homebrew/bin/aws_completer . AWS CLI Configuration Set up your AWS credentials: aws configure You'll be prompted for: AWS Access Key ID AWS Secret Access Key Default region name (e.g., us-east-1) Default output format (json, text, or table) Tip You can set up multiple profiles using aws configure --profile profilename CLI Usage Examples List S3 Buckets aws s3 ls List EC2 Instances aws ec2 describe-instances Create an IAM User aws iam create-user --user-name newuser Using JSON Parameters For complex API calls, you can use JSON files: aws ec2 run-instances --cli-input-json file://ec2-params.json JMESPath Querying Use --query to filter output: aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,State.Name]' AWS CLI Aliases Create shortcuts for common commands in ~/.aws/cli/alias : [toplevel] todays-instances = ec2 describe-instances --query 'Reservations[].Instances[?LaunchTime>=`2023-01-01`]' Use with: aws todays-instances References AWS CLI Command Reference S3 CLI Reference EC2 CLI Reference IAM CLI Reference Glacier CLI Reference","title":"Amazon Web Services"},{"location":"amazon-web-services/#amazon-web-services","text":"This guide covers setting up and using the AWS Command Line Interface (CLI) for interacting with various AWS services. S3 (Simple Storage Service) EC2 (Elastic Compute Cloud) IAM (Identity and Access Management)","title":"Amazon Web Services"},{"location":"amazon-web-services/#aws-cli-installation","text":"For fresh Ubuntu instances: curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip && sudo ./aws/install && rm -rf ./awscliv2.zip For other operating systems, refer to the official installation guide .","title":"AWS CLI Installation"},{"location":"amazon-web-services/#shell-completion-setup","text":"Set up shell completion for the AWS CLI so you can use tab to autocomplete commands and options.","title":"Shell Completion Setup"},{"location":"amazon-web-services/#bash","text":"Add to your ~/.bashrc : complete -C '/usr/local/bin/aws_completer' aws","title":"Bash"},{"location":"amazon-web-services/#zsh","text":"Add to your ~/.zshrc : autoload bashcompinit && bashcompinit autoload -Uz compinit && compinit complete -C '/usr/local/bin/aws_completer' aws Important If you installed the AWS CLI in a different location, update the path accordingly. For example, if you installed it using homebrew, the path might be /opt/homebrew/bin/aws_completer .","title":"Zsh"},{"location":"amazon-web-services/#aws-cli-configuration","text":"Set up your AWS credentials: aws configure You'll be prompted for: AWS Access Key ID AWS Secret Access Key Default region name (e.g., us-east-1) Default output format (json, text, or table) Tip You can set up multiple profiles using aws configure --profile profilename","title":"AWS CLI Configuration"},{"location":"amazon-web-services/#cli-usage-examples","text":"","title":"CLI Usage Examples"},{"location":"amazon-web-services/#list-s3-buckets","text":"aws s3 ls","title":"List S3 Buckets"},{"location":"amazon-web-services/#list-ec2-instances","text":"aws ec2 describe-instances","title":"List EC2 Instances"},{"location":"amazon-web-services/#create-an-iam-user","text":"aws iam create-user --user-name newuser","title":"Create an IAM User"},{"location":"amazon-web-services/#using-json-parameters","text":"For complex API calls, you can use JSON files: aws ec2 run-instances --cli-input-json file://ec2-params.json","title":"Using JSON Parameters"},{"location":"amazon-web-services/#jmespath-querying","text":"Use --query to filter output: aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,State.Name]'","title":"JMESPath Querying"},{"location":"amazon-web-services/#aws-cli-aliases","text":"Create shortcuts for common commands in ~/.aws/cli/alias : [toplevel] todays-instances = ec2 describe-instances --query 'Reservations[].Instances[?LaunchTime>=`2023-01-01`]' Use with: aws todays-instances","title":"AWS CLI Aliases"},{"location":"amazon-web-services/#references","text":"AWS CLI Command Reference S3 CLI Reference EC2 CLI Reference IAM CLI Reference Glacier CLI Reference","title":"References"},{"location":"amazon-web-services/ec2/","text":"EC2 EC2 instances are virtual servers that you can provision and manage in the cloud. Instance Types EC2 provides a wide range of instance types optimized for various use cases: General Purpose Compute Optimized Memory Optimized Accelerated Computing Storage Optimized Choose the instance type based on your application's requirements. Connecting to EC2 Instances SSH To connect via SSH, ensure you have the private key associated with the instance: chmod 400 /path/to/private-key.pem ssh -i /path/to/private-key.pem ec2-user@ec2-xx-xxx-xxx-xxx.compute-1.amazonaws.com EC2 Serial Console For instances that support it (typically those running on AWS Nitro System): Go to the EC2 Console Select the instance Click Connect > EC2 Serial Console > Connect Note If no login prompt appears, try pressing Enter . Quickstart Fresh EC2 instances don't come with aws cli: CLI Tools for EC2 Interaction SSH Config Optimize your SSH connections by editing ~/.ssh/config : Host aws-instance Hostname ec2-xx-xxx-xxx-xxx.compute-1.amazonaws.com User ec2-user IdentityFile ~/.ssh/aws-key.pem Now connect with: ssh aws-instance SCP (Secure Copy) Transfer files to/from your EC2 instance: # Local to EC2 scp localfile.txt aws-instance:~/ # EC2 to Local scp aws-instance:~/remotefile.txt . Rsync Synchronize directories: rsync -avz --progress /local/dir/ aws-instance:/remote/dir/ Tmux For persistent sessions, use tmux. Connect with: ssh -t aws-instance tmux attach Vim Edit remote files directly: vim scp://aws-instance//path/to/remote/file Copy current buffer to EC2: :!scp % aws-instance:~/path/to/remote/file Resources For more information, refer to the EC2 User Guide","title":"EC2"},{"location":"amazon-web-services/ec2/#ec2","text":"EC2 instances are virtual servers that you can provision and manage in the cloud.","title":"EC2"},{"location":"amazon-web-services/ec2/#instance-types","text":"EC2 provides a wide range of instance types optimized for various use cases: General Purpose Compute Optimized Memory Optimized Accelerated Computing Storage Optimized Choose the instance type based on your application's requirements.","title":"Instance Types"},{"location":"amazon-web-services/ec2/#connecting-to-ec2-instances","text":"","title":"Connecting to EC2 Instances"},{"location":"amazon-web-services/ec2/#ssh","text":"To connect via SSH, ensure you have the private key associated with the instance: chmod 400 /path/to/private-key.pem ssh -i /path/to/private-key.pem ec2-user@ec2-xx-xxx-xxx-xxx.compute-1.amazonaws.com","title":"SSH"},{"location":"amazon-web-services/ec2/#ec2-serial-console","text":"For instances that support it (typically those running on AWS Nitro System): Go to the EC2 Console Select the instance Click Connect > EC2 Serial Console > Connect Note If no login prompt appears, try pressing Enter .","title":"EC2 Serial Console"},{"location":"amazon-web-services/ec2/#quickstart","text":"Fresh EC2 instances don't come with aws cli:","title":"Quickstart"},{"location":"amazon-web-services/ec2/#cli-tools-for-ec2-interaction","text":"","title":"CLI Tools for EC2 Interaction"},{"location":"amazon-web-services/ec2/#ssh-config","text":"Optimize your SSH connections by editing ~/.ssh/config : Host aws-instance Hostname ec2-xx-xxx-xxx-xxx.compute-1.amazonaws.com User ec2-user IdentityFile ~/.ssh/aws-key.pem Now connect with: ssh aws-instance","title":"SSH Config"},{"location":"amazon-web-services/ec2/#scp-secure-copy","text":"Transfer files to/from your EC2 instance: # Local to EC2 scp localfile.txt aws-instance:~/ # EC2 to Local scp aws-instance:~/remotefile.txt .","title":"SCP (Secure Copy)"},{"location":"amazon-web-services/ec2/#rsync","text":"Synchronize directories: rsync -avz --progress /local/dir/ aws-instance:/remote/dir/","title":"Rsync"},{"location":"amazon-web-services/ec2/#tmux","text":"For persistent sessions, use tmux. Connect with: ssh -t aws-instance tmux attach","title":"Tmux"},{"location":"amazon-web-services/ec2/#vim","text":"Edit remote files directly: vim scp://aws-instance//path/to/remote/file Copy current buffer to EC2: :!scp % aws-instance:~/path/to/remote/file","title":"Vim"},{"location":"amazon-web-services/ec2/#resources","text":"For more information, refer to the EC2 User Guide","title":"Resources"},{"location":"amazon-web-services/iam/","text":"IAM Use IAM to securely manage access to AWS services and resources. Users To create a user: Navigate to IAM in the AWS Management Console Select Users from the left sidebar Click Create user Follow the prompts to set up the user Caution Providing user access to the AWS Management Console is a security risk. If you must provide access: Create an auto-generated password Require the user to change it upon first login Enable Multi-Factor Authentication (MFA) Best practice: Don't attach policies directly to users. Use groups to manage permissions instead. Example: Creating a user with AWS CLI aws iam create-user --user-name johndoe aws iam create-login-profile --user-name johndoe --password \"InitialPassword123!\" --password-reset-required Groups and Policies Groups simplify permission management by allowing you to assign policies to multiple users at once. We have three custom groups: Group Name Policies Use Case LabAdmin AdministratorAccess For administrators who need full access LabUser FullAccess to IAM, CloudShell, EC2, S3, and Glacier For regular lab users with broad but limited access LabGuest AmazonS3ReadOnlyAccess, AmazonGlacierFullAccess, IAMUserChangePassword For collaborators who need minimal access Creating a Group Go to IAM > User groups > Create group Name the group and attach relevant policies Add users to the group Example: Creating a group with AWS CLI aws iam create-group --group-name LabUser aws iam attach-group-policy --group-name LabUser --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess aws iam add-user-to-group --user-name johndoe --group-name LabUser Policies Policies define permissions for AWS resources. They can be AWS-managed, customer-managed, or inline. Inline Policies Inline policies are directly attached to a user, group, or role. They're useful for one-off permissions. Example: Allowing access to EC2 Serial Console { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowInstanceBasedSerialConsoleAccess\", \"Effect\": \"Allow\", \"Action\": [\"ec2-instance-connect:SendSerialConsoleSSHPublicKey\"], \"Resource\": \"*\" } ] } Creating a Custom Policy Go to IAM > Policies > Create policy Use the visual editor or JSON editor to define permissions Review and create the policy More about policy syntax: IAM Policy Reference Monitoring and Auditing Use AWS CloudTrail to log IAM and AWS account activity. Set up CloudWatch alarms to alert on suspicious activity. Example CloudWatch alarm for failed console sign-in attempts: { \"MetricName\": \"ConsoleSignInFailures\", \"Namespace\": \"AWS/IAM\", \"Statistic\": \"Sum\", \"Period\": 300, \"EvaluationPeriods\": 1, \"Threshold\": 3, \"ComparisonOperator\": \"GreaterThanThreshold\", \"AlarmActions\": [\"arn:aws:sns:us-east-1:123456789012:SecurityNotifications\"] } Best Practices Follow the principle of least privilege Use groups for permission management Enable MFA for all users Regularly audit and remove unused users, groups, and policies Use AWS Organizations for multi-account environments Implement a strong password policy For more information, refer to the IAM User Guide .","title":"IAM"},{"location":"amazon-web-services/iam/#iam","text":"Use IAM to securely manage access to AWS services and resources.","title":"IAM"},{"location":"amazon-web-services/iam/#users","text":"To create a user: Navigate to IAM in the AWS Management Console Select Users from the left sidebar Click Create user Follow the prompts to set up the user Caution Providing user access to the AWS Management Console is a security risk. If you must provide access: Create an auto-generated password Require the user to change it upon first login Enable Multi-Factor Authentication (MFA) Best practice: Don't attach policies directly to users. Use groups to manage permissions instead.","title":"Users"},{"location":"amazon-web-services/iam/#example-creating-a-user-with-aws-cli","text":"aws iam create-user --user-name johndoe aws iam create-login-profile --user-name johndoe --password \"InitialPassword123!\" --password-reset-required","title":"Example: Creating a user with AWS CLI"},{"location":"amazon-web-services/iam/#groups-and-policies","text":"Groups simplify permission management by allowing you to assign policies to multiple users at once. We have three custom groups: Group Name Policies Use Case LabAdmin AdministratorAccess For administrators who need full access LabUser FullAccess to IAM, CloudShell, EC2, S3, and Glacier For regular lab users with broad but limited access LabGuest AmazonS3ReadOnlyAccess, AmazonGlacierFullAccess, IAMUserChangePassword For collaborators who need minimal access","title":"Groups and Policies"},{"location":"amazon-web-services/iam/#creating-a-group","text":"Go to IAM > User groups > Create group Name the group and attach relevant policies Add users to the group","title":"Creating a Group"},{"location":"amazon-web-services/iam/#example-creating-a-group-with-aws-cli","text":"aws iam create-group --group-name LabUser aws iam attach-group-policy --group-name LabUser --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess aws iam add-user-to-group --user-name johndoe --group-name LabUser","title":"Example: Creating a group with AWS CLI"},{"location":"amazon-web-services/iam/#policies","text":"Policies define permissions for AWS resources. They can be AWS-managed, customer-managed, or inline.","title":"Policies"},{"location":"amazon-web-services/iam/#inline-policies","text":"Inline policies are directly attached to a user, group, or role. They're useful for one-off permissions. Example: Allowing access to EC2 Serial Console { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowInstanceBasedSerialConsoleAccess\", \"Effect\": \"Allow\", \"Action\": [\"ec2-instance-connect:SendSerialConsoleSSHPublicKey\"], \"Resource\": \"*\" } ] }","title":"Inline Policies"},{"location":"amazon-web-services/iam/#creating-a-custom-policy","text":"Go to IAM > Policies > Create policy Use the visual editor or JSON editor to define permissions Review and create the policy More about policy syntax: IAM Policy Reference","title":"Creating a Custom Policy"},{"location":"amazon-web-services/iam/#monitoring-and-auditing","text":"Use AWS CloudTrail to log IAM and AWS account activity. Set up CloudWatch alarms to alert on suspicious activity. Example CloudWatch alarm for failed console sign-in attempts: { \"MetricName\": \"ConsoleSignInFailures\", \"Namespace\": \"AWS/IAM\", \"Statistic\": \"Sum\", \"Period\": 300, \"EvaluationPeriods\": 1, \"Threshold\": 3, \"ComparisonOperator\": \"GreaterThanThreshold\", \"AlarmActions\": [\"arn:aws:sns:us-east-1:123456789012:SecurityNotifications\"] }","title":"Monitoring and Auditing"},{"location":"amazon-web-services/iam/#best-practices","text":"Follow the principle of least privilege Use groups for permission management Enable MFA for all users Regularly audit and remove unused users, groups, and policies Use AWS Organizations for multi-account environments Implement a strong password policy For more information, refer to the IAM User Guide .","title":"Best Practices"},{"location":"amazon-web-services/s3/","text":"S3 Amazon S3 (Simple Storage Service) is an object storage service offering industry-leading scalability, data availability, security, and performance. Operation Command List Buckets aws s3 ls Create a Bucket aws s3 mb s3://my-new-bucket Upload a File aws s3 cp localfile.txt s3://my-bucket/ Download a File aws s3 cp s3://my-bucket/remotefile.txt . Sync Files aws s3 sync <source> <target> [--options] Note: For the sync command, replace <source> and <target> with appropriate local or S3 paths, and add any necessary options. Options include: Option Description --exclude Exclude files matching the pattern --include Include files matching the pattern --dryrun Simulate the sync operation --delete Remove files in target not present in source --quiet Suppress non-error output Example: aws s3 sync . s3://my-bucket/backup --exclude \"*.tmp\" --delete Tip Use the \"Copy S3 URI\" button in the S3 console to easily copy bucket/object URIs. Working with S3 Objects Rename/Move Objects While mv exists, it's not atomic. Use cp followed by rm for better control: aws s3 cp s3://my-bucket/oldname s3://my-bucket/newname \\ && aws s3 rm s3://my-bucket/oldname List Objects with a Specific Prefix aws s3 ls s3://my-bucket/prefix/ Delete Multiple Objects aws s3 rm s3://my-bucket/prefix --recursive S3 Storage Classes S3 offers various storage classes for different use cases: Standard Intelligent-Tiering Glacier Instant Retrieval Glacier Deep Archive Working with Glacier Storage Class To restore a Glacier object: aws s3api restore-object --bucket my-bucket --key path/to/object --restore-request '{\"Days\":5,\"GlacierJobParameters\":{\"Tier\":\"Standard\"}}' documentation Check restore status: aws s3api head-object --bucket my-bucket --key path/to/object When downloading restored Glacier objects: aws s3 cp s3://my-bucket/path/to/object . --force-glacier-transfer Mounting S3 as a Filesystem Using mountpoint-s3 : Install: On Ubuntu: wget https://s3.amazonaws.com/mountpoint-s3-release/latest/x86_64/mount-s3.deb sudo apt-get install ./mount-s3.deb -y && rm mount-s3.deb On macOS: brew install --cask macfuse # install the prerequisite FUSE library wget https://s3.amazonaws.com/mountpoint-s3-release/latest/arm64/mount-s3.tar.gz sudo mkdir -p /opt/aws/mountpoint-s3 && sudo tar -C /opt/aws/mountpoint-s3 -xzf ./mount-s3.tar.gz export PATH=$PATH:/opt/aws/mountpoint-s3/bin # add to .zshrc Mount: mkdir -vp /mnt/s3/my-bucket mount-s3 my-bucket /mnt/s3/my-bucket For more information, refer to the Mountpoint S3 documentation . Quick start Create mountpoints for each bucket you have access to: for bucket in $(aws s3 ls | awk '{print $3}'); do mkdir -vp ./s3/$bucket mount-s3 $bucket ./s3/$bucket done Caution This command will mount all buckets you have access to in the present working directory. Ensure you have the necessary permissions and that this is where you want to mount the buckets.","title":"S3"},{"location":"amazon-web-services/s3/#s3","text":"Amazon S3 (Simple Storage Service) is an object storage service offering industry-leading scalability, data availability, security, and performance. Operation Command List Buckets aws s3 ls Create a Bucket aws s3 mb s3://my-new-bucket Upload a File aws s3 cp localfile.txt s3://my-bucket/ Download a File aws s3 cp s3://my-bucket/remotefile.txt . Sync Files aws s3 sync <source> <target> [--options] Note: For the sync command, replace <source> and <target> with appropriate local or S3 paths, and add any necessary options. Options include: Option Description --exclude Exclude files matching the pattern --include Include files matching the pattern --dryrun Simulate the sync operation --delete Remove files in target not present in source --quiet Suppress non-error output Example: aws s3 sync . s3://my-bucket/backup --exclude \"*.tmp\" --delete Tip Use the \"Copy S3 URI\" button in the S3 console to easily copy bucket/object URIs.","title":"S3"},{"location":"amazon-web-services/s3/#working-with-s3-objects","text":"","title":"Working with S3 Objects"},{"location":"amazon-web-services/s3/#renamemove-objects","text":"While mv exists, it's not atomic. Use cp followed by rm for better control: aws s3 cp s3://my-bucket/oldname s3://my-bucket/newname \\ && aws s3 rm s3://my-bucket/oldname","title":"Rename/Move Objects"},{"location":"amazon-web-services/s3/#list-objects-with-a-specific-prefix","text":"aws s3 ls s3://my-bucket/prefix/","title":"List Objects with a Specific Prefix"},{"location":"amazon-web-services/s3/#delete-multiple-objects","text":"aws s3 rm s3://my-bucket/prefix --recursive","title":"Delete Multiple Objects"},{"location":"amazon-web-services/s3/#s3-storage-classes","text":"S3 offers various storage classes for different use cases: Standard Intelligent-Tiering Glacier Instant Retrieval Glacier Deep Archive","title":"S3 Storage Classes"},{"location":"amazon-web-services/s3/#working-with-glacier-storage-class","text":"To restore a Glacier object: aws s3api restore-object --bucket my-bucket --key path/to/object --restore-request '{\"Days\":5,\"GlacierJobParameters\":{\"Tier\":\"Standard\"}}' documentation Check restore status: aws s3api head-object --bucket my-bucket --key path/to/object When downloading restored Glacier objects: aws s3 cp s3://my-bucket/path/to/object . --force-glacier-transfer","title":"Working with Glacier Storage Class"},{"location":"amazon-web-services/s3/#mounting-s3-as-a-filesystem","text":"Using mountpoint-s3 : Install: On Ubuntu: wget https://s3.amazonaws.com/mountpoint-s3-release/latest/x86_64/mount-s3.deb sudo apt-get install ./mount-s3.deb -y && rm mount-s3.deb On macOS: brew install --cask macfuse # install the prerequisite FUSE library wget https://s3.amazonaws.com/mountpoint-s3-release/latest/arm64/mount-s3.tar.gz sudo mkdir -p /opt/aws/mountpoint-s3 && sudo tar -C /opt/aws/mountpoint-s3 -xzf ./mount-s3.tar.gz export PATH=$PATH:/opt/aws/mountpoint-s3/bin # add to .zshrc Mount: mkdir -vp /mnt/s3/my-bucket mount-s3 my-bucket /mnt/s3/my-bucket For more information, refer to the Mountpoint S3 documentation .","title":"Mounting S3 as a Filesystem"},{"location":"amazon-web-services/s3/#quick-start","text":"Create mountpoints for each bucket you have access to: for bucket in $(aws s3 ls | awk '{print $3}'); do mkdir -vp ./s3/$bucket mount-s3 $bucket ./s3/$bucket done Caution This command will mount all buckets you have access to in the present working directory. Ensure you have the necessary permissions and that this is where you want to mount the buckets.","title":"Quick start"},{"location":"command-line/","text":"Linux and the Command Line Linux is a family of open-source Unix-like operating systems based on the Linux kernel. It's typically packaged in a distribution, which includes the Linux kernel along with a set of software tools and utilities. There are many different distributions of Linux, such as Ubuntu, Fedora, Red Hat, and CentOS. Linux distributions all run a shell, which is a program that interprets and executes programs. The \"command line\" refers to the interface between you and the shell. Concepts Operating System : The software that manages resources of a computer. Architecture : The type of CPU (e.g., x86, x86_64, ARM). Distribution : A version of the Linux operating system. Terminal : The window in which you type commands. Shell : The program that interprets and executes commands. Important When creating an instance on AWS, you're setting up a virtual machine that > runs an operating system of your choice. > For maximum compatibility with our software, use Ubuntu 22.04 LTS (on 64-bit x86). Terminal The \"terminal\" is the interface between you and the shell. It's also referred to as the \"command line\" or the \"console\". Various programs can serve as a terminal, such as: iTerm on macOS cmd on Windows EC2 Serial Console The \"Terminal\" tab in RStudio Shell The \"shell\" is the program that interprets and executes commands. Examples include: bash (most common on Linux systems) zsh (the default on macOS) Tip For maintainability and portability, it's recommended to write scripts in bash and be explicit about it. For an interesting discussion on this topic, check out this post . Read through the bash guide for more information on the writing and executing shell scripts. Windows Subsystem for Linux (WSL) Windows users can run a Linux distribution on Windows using WSL. This allows you to run Linux command-line tools and utilities directly on Windows without needing a virtual machine or dual-boot setup. If you want to use WSL, follow the installation instructions . Linux Command Line Interface (CLI) To check your bash version: bash --version Example output: GNU bash, version 5.1.16(1)-release (x86_64-pc-linux-gnu) Copyright (C) 2020 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html> To verify if bash is running: echo $SHELL User Management Set a password for the ubuntu user sudo passwd ubuntu Adding a user to sudo ers sudo adduser username sudo Configure no-password sudo Open the sudo ers file for editing: sudo visudo [!NOTE] > visudo is a special command that edits the sudo ers file safely. If the environment variable $EDITOR is not set, it will use vi . Uncomment or add these lines: # Members of the admin group may gain root privileges %admin ALL=(ALL) ALL # Allow members of group sudo to execute any command %sudo ALL=(ALL:ALL) ALL # Add to run any command without a password username ALL=(ALL) NOPASSWD: ALL Quick Reference of common Linux commands Command Description pwd p rint w orking d irectory ls l i s t files and directories in the current directory cd c hange d irectory cp c o p y files or directories mv m o v e or rename files or directories touch create an empty file mkdir m a k e a new dir ectory rm r e m ove files or directories wc w ord c ount less view the contents of a file head view the first few lines of a file tail view the last few lines of a file gzip compress a file gunzip decompress a file curl download the contents of a website echo print text to the terminal cat concatenate and print files man view the man ual for a command","title":"Linux and the Command Line"},{"location":"command-line/#linux-and-the-command-line","text":"Linux is a family of open-source Unix-like operating systems based on the Linux kernel. It's typically packaged in a distribution, which includes the Linux kernel along with a set of software tools and utilities. There are many different distributions of Linux, such as Ubuntu, Fedora, Red Hat, and CentOS. Linux distributions all run a shell, which is a program that interprets and executes programs. The \"command line\" refers to the interface between you and the shell.","title":"Linux and the Command Line"},{"location":"command-line/#concepts","text":"Operating System : The software that manages resources of a computer. Architecture : The type of CPU (e.g., x86, x86_64, ARM). Distribution : A version of the Linux operating system. Terminal : The window in which you type commands. Shell : The program that interprets and executes commands. Important When creating an instance on AWS, you're setting up a virtual machine that > runs an operating system of your choice. > For maximum compatibility with our software, use Ubuntu 22.04 LTS (on 64-bit x86).","title":"Concepts"},{"location":"command-line/#terminal","text":"The \"terminal\" is the interface between you and the shell. It's also referred to as the \"command line\" or the \"console\". Various programs can serve as a terminal, such as: iTerm on macOS cmd on Windows EC2 Serial Console The \"Terminal\" tab in RStudio","title":"Terminal"},{"location":"command-line/#shell","text":"The \"shell\" is the program that interprets and executes commands. Examples include: bash (most common on Linux systems) zsh (the default on macOS) Tip For maintainability and portability, it's recommended to write scripts in bash and be explicit about it. For an interesting discussion on this topic, check out this post . Read through the bash guide for more information on the writing and executing shell scripts.","title":"Shell"},{"location":"command-line/#windows-subsystem-for-linux-wsl","text":"Windows users can run a Linux distribution on Windows using WSL. This allows you to run Linux command-line tools and utilities directly on Windows without needing a virtual machine or dual-boot setup. If you want to use WSL, follow the installation instructions .","title":"Windows Subsystem for Linux (WSL)"},{"location":"command-line/#linux-command-line-interface-cli","text":"To check your bash version: bash --version Example output: GNU bash, version 5.1.16(1)-release (x86_64-pc-linux-gnu) Copyright (C) 2020 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html> To verify if bash is running: echo $SHELL","title":"Linux Command Line Interface (CLI)"},{"location":"command-line/#user-management","text":"","title":"User Management"},{"location":"command-line/#set-a-password-for-the-ubuntu-user","text":"sudo passwd ubuntu","title":"Set a password for the ubuntu user"},{"location":"command-line/#adding-a-user-to-sudoers","text":"sudo adduser username sudo","title":"Adding a user to sudoers"},{"location":"command-line/#configure-no-password-sudo","text":"Open the sudo ers file for editing: sudo visudo [!NOTE] > visudo is a special command that edits the sudo ers file safely. If the environment variable $EDITOR is not set, it will use vi . Uncomment or add these lines: # Members of the admin group may gain root privileges %admin ALL=(ALL) ALL # Allow members of group sudo to execute any command %sudo ALL=(ALL:ALL) ALL # Add to run any command without a password username ALL=(ALL) NOPASSWD: ALL","title":"Configure no-password sudo"},{"location":"command-line/#quick-reference-of-common-linux-commands","text":"Command Description pwd p rint w orking d irectory ls l i s t files and directories in the current directory cd c hange d irectory cp c o p y files or directories mv m o v e or rename files or directories touch create an empty file mkdir m a k e a new dir ectory rm r e m ove files or directories wc w ord c ount less view the contents of a file head view the first few lines of a file tail view the last few lines of a file gzip compress a file gunzip decompress a file curl download the contents of a website echo print text to the terminal cat concatenate and print files man view the man ual for a command","title":"Quick Reference of common Linux commands"},{"location":"command-line/bash/","text":"Bash Scripting Google recommends using bash for all executable shell scripts: Restricting all executable shell scripts to bash gives us a consistent shell language that's installed on all our machines. In particular, this means there is generally no need to strive for POSIX-compatibility or otherwise avoid \"bashisms\". Start with a shebang The shebang is the first line of a script and tells the system which interpreter to use to run the script. For bash scripts, use #!/bin/bash : #!/bin/bash # After the shebang, add comments to describe the script echo \"Hello, world!\" You may see other shebangs like #!/bin/sh or #!/usr/bin/env bash . The former uses the system's default shell, which may not be bash . The latter is more portable as it can find bash in different locations. Executing scripts To execute a script from the command line, you can manually invoke the interpreter with bash <filename> or you can make the file executable: chmod +x /path/to/script.sh Note If you use the command ls -l , you'll see an x in the permissions column for the script. More about that here . Then run it: ./path/to/script.sh Tip The ./ prefix is required to run scripts in the current directory. Try saving the \"Hello, world!\" script above to a file and running it after making it executable. (The .sh extension is optional.) Resources man bash Bash Reference Manual Bash Guide Advanced Bash-Scripting Guide Safety First! Bashisms Bash Pitfalls Arrays in Bash Shell Script Templates Command Name Extensions Considered Harmful For a quick reference, check out DevHints.io . Special Variables and Commands Special variables and commands can be very useful in bash scripts, particularly for interacting with command history: Command Description !! Repeat the last command. !$ Last argument of the last command. $_ Last argument of the last command. !^ First argument of the last command. $? Exit status of the last command. !n Repeat command number n from history. !!:n The n -th argument of the last command. !!:s/old/new/ Substitute old with new in the last command. !cmd Run the most recent command starting with cmd . !+ Next command in the history list (after the current one). !-n The command n lines before the current command in history. !!:p Print the last command without executing it. !!:x Extract and display the x -th argument of the last command. Examples You ran a command without sudo and want to run it again with sudo : $ apt update E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied) $ sudo !! You performed a dry run with echo and want to run the command for real: echo sudo apt-get install -y vim !:2-* Update and Upgrade Everything on Ubuntu To update and upgrade all packages: sudo apt-get update && sudo apt-get upgrade -y Alternatively: yes | sudo sh -c 'apt update && apt upgrade && apt dist-upgrade \\ && apt autoremove && apt autoclean && apt clean' Special text inside bash scripts Multi-line Comments To add multi-line comments in bash , use the following syntax: : ' This is a very neat comment in bash ' Here Documents A here document allows you to pass multiple lines of input to a command without storing it in a file: cat <<EOF This is a here document. It can be used to pass multiple lines of input to a command. EOF More useful commands Command Description ssh user@host 'bash -s' < script.sh Run a local script on a remote machine. md5sum ./*.fastq.gz > md5sums.txt Generate a file with the MD5 checksums of all files in the current directory. md5sum -c md5sums.txt Check files against the MD5 checksums in a file. du -ha --max-depth=1 Check the total size of each folder in the current directory. free -h Check current memory usage. Loop across files of the same extension Don't use ls or find . Instead, use a glob pattern: for file in ./*.bam; do # Loop across all .bsudoam files in the current directory command \"$file\" # Quote the variable to handle spaces in filenames done # single-line loop for file in ./*.bam; do command \"$file\"; done source Safely change directories Don't cd without checking if it was successful! You might end up executing commands in the wrong directory. cd /path/to/some/dir || exit # Exit if the directory doesn't exist or is inaccessible rm -rf * # This could be very bad if the cd failed source Correctly define functions Don't use function to define functions! It's ugly and not portable. Just use the function name followed by parentheses. We'll know it's a function. # bad function myfunc { echo \"Hello, world!\" } # very bad function myfunc() { echo \"Hello, world!\" } # good myfunc() { echo \"Hello, world!\" } source Tip To define single-line functions, add a semicolon before the closing brace: myfunc() { echo \"Hello, world!\"; }","title":"Bash Scripting"},{"location":"command-line/bash/#bash-scripting","text":"Google recommends using bash for all executable shell scripts: Restricting all executable shell scripts to bash gives us a consistent shell language that's installed on all our machines. In particular, this means there is generally no need to strive for POSIX-compatibility or otherwise avoid \"bashisms\".","title":"Bash Scripting"},{"location":"command-line/bash/#start-with-a-shebang","text":"The shebang is the first line of a script and tells the system which interpreter to use to run the script. For bash scripts, use #!/bin/bash : #!/bin/bash # After the shebang, add comments to describe the script echo \"Hello, world!\" You may see other shebangs like #!/bin/sh or #!/usr/bin/env bash . The former uses the system's default shell, which may not be bash . The latter is more portable as it can find bash in different locations.","title":"Start with a shebang"},{"location":"command-line/bash/#executing-scripts","text":"To execute a script from the command line, you can manually invoke the interpreter with bash <filename> or you can make the file executable: chmod +x /path/to/script.sh Note If you use the command ls -l , you'll see an x in the permissions column for the script. More about that here . Then run it: ./path/to/script.sh Tip The ./ prefix is required to run scripts in the current directory. Try saving the \"Hello, world!\" script above to a file and running it after making it executable. (The .sh extension is optional.)","title":"Executing scripts"},{"location":"command-line/bash/#resources","text":"man bash Bash Reference Manual Bash Guide Advanced Bash-Scripting Guide Safety First! Bashisms Bash Pitfalls Arrays in Bash Shell Script Templates Command Name Extensions Considered Harmful For a quick reference, check out DevHints.io .","title":"Resources"},{"location":"command-line/bash/#special-variables-and-commands","text":"Special variables and commands can be very useful in bash scripts, particularly for interacting with command history: Command Description !! Repeat the last command. !$ Last argument of the last command. $_ Last argument of the last command. !^ First argument of the last command. $? Exit status of the last command. !n Repeat command number n from history. !!:n The n -th argument of the last command. !!:s/old/new/ Substitute old with new in the last command. !cmd Run the most recent command starting with cmd . !+ Next command in the history list (after the current one). !-n The command n lines before the current command in history. !!:p Print the last command without executing it. !!:x Extract and display the x -th argument of the last command.","title":"Special Variables and Commands"},{"location":"command-line/bash/#examples","text":"You ran a command without sudo and want to run it again with sudo : $ apt update E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied) $ sudo !! You performed a dry run with echo and want to run the command for real: echo sudo apt-get install -y vim !:2-*","title":"Examples"},{"location":"command-line/bash/#update-and-upgrade-everything-on-ubuntu","text":"To update and upgrade all packages: sudo apt-get update && sudo apt-get upgrade -y Alternatively: yes | sudo sh -c 'apt update && apt upgrade && apt dist-upgrade \\ && apt autoremove && apt autoclean && apt clean'","title":"Update and Upgrade Everything on Ubuntu"},{"location":"command-line/bash/#special-text-inside-bash-scripts","text":"","title":"Special text inside bash scripts"},{"location":"command-line/bash/#multi-line-comments","text":"To add multi-line comments in bash , use the following syntax: : ' This is a very neat comment in bash '","title":"Multi-line Comments"},{"location":"command-line/bash/#here-documents","text":"A here document allows you to pass multiple lines of input to a command without storing it in a file: cat <<EOF This is a here document. It can be used to pass multiple lines of input to a command. EOF","title":"Here Documents"},{"location":"command-line/bash/#more-useful-commands","text":"Command Description ssh user@host 'bash -s' < script.sh Run a local script on a remote machine. md5sum ./*.fastq.gz > md5sums.txt Generate a file with the MD5 checksums of all files in the current directory. md5sum -c md5sums.txt Check files against the MD5 checksums in a file. du -ha --max-depth=1 Check the total size of each folder in the current directory. free -h Check current memory usage.","title":"More useful commands"},{"location":"command-line/bash/#loop-across-files-of-the-same-extension","text":"Don't use ls or find . Instead, use a glob pattern: for file in ./*.bam; do # Loop across all .bsudoam files in the current directory command \"$file\" # Quote the variable to handle spaces in filenames done # single-line loop for file in ./*.bam; do command \"$file\"; done source","title":"Loop across files of the same extension"},{"location":"command-line/bash/#safely-change-directories","text":"Don't cd without checking if it was successful! You might end up executing commands in the wrong directory. cd /path/to/some/dir || exit # Exit if the directory doesn't exist or is inaccessible rm -rf * # This could be very bad if the cd failed source","title":"Safely change directories"},{"location":"command-line/bash/#correctly-define-functions","text":"Don't use function to define functions! It's ugly and not portable. Just use the function name followed by parentheses. We'll know it's a function. # bad function myfunc { echo \"Hello, world!\" } # very bad function myfunc() { echo \"Hello, world!\" } # good myfunc() { echo \"Hello, world!\" } source Tip To define single-line functions, add a semicolon before the closing brace: myfunc() { echo \"Hello, world!\"; }","title":"Correctly define functions"},{"location":"command-line/file-types/","text":"File Types File Type Description Typical Extension FASTQ Raw reads from sequencer (often compressed) .fastq, .fastq.gz FASTA Sequence data .fasta, .fa, .fna GTF Gene Transfer Format (GFF2 variant) .gtf, .gtf.gz GFF General Feature Format .gff3, .gff3.gz SAM Sequence Alignment/Map .sam, .sam.gz BAM Binary Alignment/Map .bam CRAM Compressed Reference-oriented Alignment Map .cram VCF Variant Call Format .vcf, BED Browser Extensible Data .bed, TSV Tab-Separated Values .tsv, CSV Comma-Separated Values .csv, JSON JavaScript Object Notation .json, Markdown Markup language for documentation .md, .markdown TXT Plain Text .txt, md5 Checksum for file integrity .md5, .txt","title":"File Types"},{"location":"command-line/file-types/#file-types","text":"File Type Description Typical Extension FASTQ Raw reads from sequencer (often compressed) .fastq, .fastq.gz FASTA Sequence data .fasta, .fa, .fna GTF Gene Transfer Format (GFF2 variant) .gtf, .gtf.gz GFF General Feature Format .gff3, .gff3.gz SAM Sequence Alignment/Map .sam, .sam.gz BAM Binary Alignment/Map .bam CRAM Compressed Reference-oriented Alignment Map .cram VCF Variant Call Format .vcf, BED Browser Extensible Data .bed, TSV Tab-Separated Values .tsv, CSV Comma-Separated Values .csv, JSON JavaScript Object Notation .json, Markdown Markup language for documentation .md, .markdown TXT Plain Text .txt, md5 Checksum for file integrity .md5, .txt","title":"File Types"},{"location":"command-line/perl/","text":"Perl Cheatsheet Perl is a high-level, general-purpose, interpreted, dynamic programming language. Use perl over bash for more complex scripts. CONTEXTS SIGILS ref ARRAYS HASHES void $scalar SCALAR @array %hash scalar @array ARRAY @array[0, 2] @hash{'a', 'b'} list %hash HASH $array[0] $hash{'a'} &sub CODE *glob GLOB SCALAR VALUES FORMAT number, string, ref, glob, undef REFERENCES \\ reference $$foo[1] aka $foo->[1] $@%&* dereference $$foo{bar} aka $foo->{bar} [] anon. arrayref ${$$foo[1]}[2] aka $foo->[1]->[2] {} anon. hashref ${$$foo[1]}[2] aka $foo->[1][2] \\() list of refs SYNTAX OPERATOR PRECEDENCE foreach (LIST) { } for (a;b;c) { } -> while (e) { } until (e) { } ++ -- if (e) { } elsif (e) { } else { } ** unless (e) { } elsif (e) { } else { } ! ~ \\ u+ u- given (e) { when (e) {} default {} } =~ !~ * / % x NUMBERS vs STRINGS FALSE vs TRUE + - . = = undef, \"\", 0, \"0\" << >> + . anything else named uops == != eq ne < > <= >= lt gt le ge < > <= >= lt gt le ge == != <=> eq ne cmp ~~ <=> cmp & | ^ REGEX MODIFIERS REGEX METACHARS && /i case insensitive ^ string begin || // /m line based ^$ $ str end (bfr \\n) .. ... /s . includes \\n + one or more ?: /x /xx ign. wh.space * zero or more = += last goto /p preserve ? zero or one , => /a ASCII /aa safe {3,7} repeat in range list ops /l locale /d dual | alternation not /u Unicode [] character class and /e evaluate /ee rpts \\b boundary or xor /g global \\z string end /o compile pat once () capture DEBUG (?:p) no capture -MO=Deparse REGEX CHARCLASSES (?#t) comment -MO=Terse . [^\\n] (?=p) ZW pos ahead -D## \\s whitespace (?!p) ZW neg ahead -d:Trace \\w word chars (?<=p) ZW pos behind \\K \\d digits (?<!p) ZW neg behind CONFIGURATION \\pP named property (?>p) no backtrack perl -V:ivsize \\h horiz.wh.space (?|p|p)branch reset \\R linebreak (?<n>p)named capture \\S \\W \\D \\H negate \\g{n} ref to named cap \\K keep left part FUNCTION RETURN LISTS stat localtime caller SPECIAL VARIABLES 0 dev 0 second 0 package $_ default variable 1 ino 1 minute 1 filename $0 program name 2 mode 2 hour 2 line $/ input separator 3 nlink 3 day 3 subroutine $\\ output separator 4 uid 4 month-1 4 hasargs $| autoflush 5 gid 5 year-1900 5 wantarray $! sys/libcall error 6 rdev 6 weekday 6 evaltext $@ eval error 7 size 7 yearday 7 is_require $$ process ID 8 atime 8 is_dst 8 hints $. line number 9 mtime 9 bitmask @ARGV command line args 10 ctime 10 hinthash @INC include paths 11 blksz 3..10 only @_ subroutine args 12 blcks with EXPR %ENV environment Source to run .pl files are run by the perl interpreter $ perl script.pl Hello, Perl! script.pl #!/usr/bin/perl print \"Hello, Perl!\\n\"; more perl #!/usr/bin/perl # print the ASCII value of each character in a string $str = \"Hello, Perl!\"; for $i (0..length($str)-1) { print substr($str, $i, 1) . \" - \" . ord(substr($str, $i, 1)) . \"\\n\"; }","title":"Perl Cheatsheet"},{"location":"command-line/perl/#perl-cheatsheet","text":"Perl is a high-level, general-purpose, interpreted, dynamic programming language. Use perl over bash for more complex scripts. CONTEXTS SIGILS ref ARRAYS HASHES void $scalar SCALAR @array %hash scalar @array ARRAY @array[0, 2] @hash{'a', 'b'} list %hash HASH $array[0] $hash{'a'} &sub CODE *glob GLOB SCALAR VALUES FORMAT number, string, ref, glob, undef REFERENCES \\ reference $$foo[1] aka $foo->[1] $@%&* dereference $$foo{bar} aka $foo->{bar} [] anon. arrayref ${$$foo[1]}[2] aka $foo->[1]->[2] {} anon. hashref ${$$foo[1]}[2] aka $foo->[1][2] \\() list of refs SYNTAX OPERATOR PRECEDENCE foreach (LIST) { } for (a;b;c) { } -> while (e) { } until (e) { } ++ -- if (e) { } elsif (e) { } else { } ** unless (e) { } elsif (e) { } else { } ! ~ \\ u+ u- given (e) { when (e) {} default {} } =~ !~ * / % x NUMBERS vs STRINGS FALSE vs TRUE + - . = = undef, \"\", 0, \"0\" << >> + . anything else named uops == != eq ne < > <= >= lt gt le ge < > <= >= lt gt le ge == != <=> eq ne cmp ~~ <=> cmp & | ^ REGEX MODIFIERS REGEX METACHARS && /i case insensitive ^ string begin || // /m line based ^$ $ str end (bfr \\n) .. ... /s . includes \\n + one or more ?: /x /xx ign. wh.space * zero or more = += last goto /p preserve ? zero or one , => /a ASCII /aa safe {3,7} repeat in range list ops /l locale /d dual | alternation not /u Unicode [] character class and /e evaluate /ee rpts \\b boundary or xor /g global \\z string end /o compile pat once () capture DEBUG (?:p) no capture -MO=Deparse REGEX CHARCLASSES (?#t) comment -MO=Terse . [^\\n] (?=p) ZW pos ahead -D## \\s whitespace (?!p) ZW neg ahead -d:Trace \\w word chars (?<=p) ZW pos behind \\K \\d digits (?<!p) ZW neg behind CONFIGURATION \\pP named property (?>p) no backtrack perl -V:ivsize \\h horiz.wh.space (?|p|p)branch reset \\R linebreak (?<n>p)named capture \\S \\W \\D \\H negate \\g{n} ref to named cap \\K keep left part FUNCTION RETURN LISTS stat localtime caller SPECIAL VARIABLES 0 dev 0 second 0 package $_ default variable 1 ino 1 minute 1 filename $0 program name 2 mode 2 hour 2 line $/ input separator 3 nlink 3 day 3 subroutine $\\ output separator 4 uid 4 month-1 4 hasargs $| autoflush 5 gid 5 year-1900 5 wantarray $! sys/libcall error 6 rdev 6 weekday 6 evaltext $@ eval error 7 size 7 yearday 7 is_require $$ process ID 8 atime 8 is_dst 8 hints $. line number 9 mtime 9 bitmask @ARGV command line args 10 ctime 10 hinthash @INC include paths 11 blksz 3..10 only @_ subroutine args 12 blcks with EXPR %ENV environment Source","title":"Perl Cheatsheet"},{"location":"command-line/perl/#to-run","text":".pl files are run by the perl interpreter $ perl script.pl Hello, Perl!","title":"to run"},{"location":"command-line/perl/#scriptpl","text":"#!/usr/bin/perl print \"Hello, Perl!\\n\";","title":"script.pl"},{"location":"command-line/perl/#more-perl","text":"#!/usr/bin/perl # print the ASCII value of each character in a string $str = \"Hello, Perl!\"; for $i (0..length($str)-1) { print substr($str, $i, 1) . \" - \" . ord(substr($str, $i, 1)) . \"\\n\"; }","title":"more perl"},{"location":"command-line/ssh/","text":"SSH (Secure Shell) Check your ip address macOS ifconfig | grep inet Linux hostname -I SSH Operations Installing OpenSSH server sudo apt install openssh-server Connecting to a remote server ssh username@hostname Copying files to/from a remote server scp username@hostname:/path/to/remote/file /path/to/local/file scp /path/to/local/file username@hostname:/path/to/remote/file Setting up passwordless SSH Generate a new SSH key pair: ssh-keygen -t rsa -b 4096 -C \"a comment\" Copy the public key to the remote server: ssh-copy-id -i <identity_file >username@hostname Modify the ~/.ssh/config file: Host hostname User username HostName hostname IdentityFile ~/.ssh/identity_file Now you can log in without a password: ssh username@hostname","title":"SSH (Secure Shell)"},{"location":"command-line/ssh/#ssh-secure-shell","text":"","title":"SSH (Secure Shell)"},{"location":"command-line/ssh/#check-your-ip-address","text":"","title":"Check your ip address"},{"location":"command-line/ssh/#macos","text":"ifconfig | grep inet","title":"macOS"},{"location":"command-line/ssh/#linux","text":"hostname -I","title":"Linux"},{"location":"command-line/ssh/#ssh-operations","text":"","title":"SSH Operations"},{"location":"command-line/ssh/#installing-openssh-server","text":"sudo apt install openssh-server","title":"Installing OpenSSH server"},{"location":"command-line/ssh/#connecting-to-a-remote-server","text":"ssh username@hostname","title":"Connecting to a remote server"},{"location":"command-line/ssh/#copying-files-tofrom-a-remote-server","text":"scp username@hostname:/path/to/remote/file /path/to/local/file scp /path/to/local/file username@hostname:/path/to/remote/file","title":"Copying files to/from a remote server"},{"location":"command-line/ssh/#setting-up-passwordless-ssh","text":"Generate a new SSH key pair: ssh-keygen -t rsa -b 4096 -C \"a comment\" Copy the public key to the remote server: ssh-copy-id -i <identity_file >username@hostname Modify the ~/.ssh/config file: Host hostname User username HostName hostname IdentityFile ~/.ssh/identity_file Now you can log in without a password: ssh username@hostname","title":"Setting up passwordless SSH"},{"location":"private/","text":"Private Caution This section is intended for Palomero Lab members and the content may not be useful to the general public. Contents How to edit the lab website","title":"Private"},{"location":"private/#private","text":"Caution This section is intended for Palomero Lab members and the content may not be useful to the general public.","title":"Private"},{"location":"private/#contents","text":"How to edit the lab website","title":"Contents"},{"location":"private/lab-website/","text":"palomerolab.org Edit the description here Once you commit your changes, you can check the status here Your changes should show up in the website if the build/deployment is successful Tip If you are not seeing your changes, try refreshing the page, clearing your cache, or opening the website in a private window.","title":"palomerolab.org"},{"location":"private/lab-website/#palomerolaborg","text":"Edit the description here Once you commit your changes, you can check the status here Your changes should show up in the website if the build/deployment is successful Tip If you are not seeing your changes, try refreshing the page, clearing your cache, or opening the website in a private window.","title":"palomerolab.org"},{"location":"software/","text":"Package Management Package management is a crucial aspect of software development and scientific computing. It involves organizing, installing, updating, and removing software packages efficiently. This guide covers various package management tools and best practices for different environments. Key Concepts Package : A collection of software components bundled together. Dependency : A software package required by another package to function properly. Repository : A storage location for packages. Version Control : Managing different versions of packages. Package Management Tools We cover several package management tools in this guide: Conda and Micromamba : For managing Python and other language packages, especially useful in bioinformatics. R and renv : For managing R packages and project environments. Docker : For containerizing applications and their dependencies. micromamba Quickstart Run and accept defaults: \"${SHELL}\" <(curl -L micro.mamba.pm/install.sh) Add the bioconda channel: micromamba config add channels bioconda Best Practices Use virtual environments to isolate project dependencies. Keep your package management tool updated. Use version pinning to ensure reproducibility. Regularly update your packages, but be cautious of breaking changes. Document your project's dependencies (e.g., using requirements files or lock files). Further Reading Conda Documentation R Packages Book Docker Documentation","title":"Package Management"},{"location":"software/#package-management","text":"Package management is a crucial aspect of software development and scientific computing. It involves organizing, installing, updating, and removing software packages efficiently. This guide covers various package management tools and best practices for different environments.","title":"Package Management"},{"location":"software/#key-concepts","text":"Package : A collection of software components bundled together. Dependency : A software package required by another package to function properly. Repository : A storage location for packages. Version Control : Managing different versions of packages.","title":"Key Concepts"},{"location":"software/#package-management-tools","text":"We cover several package management tools in this guide: Conda and Micromamba : For managing Python and other language packages, especially useful in bioinformatics. R and renv : For managing R packages and project environments. Docker : For containerizing applications and their dependencies.","title":"Package Management Tools"},{"location":"software/#micromamba","text":"","title":"micromamba"},{"location":"software/#quickstart","text":"Run and accept defaults: \"${SHELL}\" <(curl -L micro.mamba.pm/install.sh) Add the bioconda channel: micromamba config add channels bioconda","title":"Quickstart"},{"location":"software/#best-practices","text":"Use virtual environments to isolate project dependencies. Keep your package management tool updated. Use version pinning to ensure reproducibility. Regularly update your packages, but be cautious of breaking changes. Document your project's dependencies (e.g., using requirements files or lock files).","title":"Best Practices"},{"location":"software/#further-reading","text":"Conda Documentation R Packages Book Docker Documentation","title":"Further Reading"},{"location":"software/Docker/","text":"Docker Docker is a platform for developing, shipping, and running applications in containers. It enables developers to package an application with all of its dependencies into a standardized unit for software development. Docker containers are lightweight and contain everything needed to run the application, including code, runtime, system tools, system libraries, and settings. Docker Installations There are two main Docker installations: Docker Desktop for Linux Docker Engine Docker Desktop is recommended for beginners as it provides a user-friendly interface and additional features. Source Installation Caution Always check the official Docker documentation for the most up-to-date instructions. The following steps are for Ubuntu 22.04. Preparing for Installation First, remove any old or unofficial versions of Docker: for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done Ubuntu Installation Set up Docker's apt repository: Follow the instructions in the official guide . Install Docker Desktop: Follow the steps outlined here . Note You may need to enable virtualization in your system's BIOS settings for Docker Desktop to work properly. Setting up pass for Credential Storage Docker uses pass to store credentials securely. Initialize it with: gpg --generate-key pass init \"Your GPG ID or email\" Installing RStudio Server via Docker To run RStudio Server in a Docker container: Pull the RStudio Server image: docker pull rocker/rstudio Run the container: docker run -d -p 8787:8787 -e PASSWORD=yourpassword --name rstudio rocker/rstudio This command: Runs the container in detached mode ( -d ) Maps port 8787 on the host to port 8787 in the container ( -p 8787:8787 ) Sets a password for the RStudio user ( -e PASSWORD=yourpassword ) Names the container \"rstudio\" ( --name rstudio ) Access RStudio Server by navigating to http://localhost:8787 in your web browser. Docker Engine Examples Building Multi-Architecture Images To build images for different architectures: Set up buildx: docker buildx create --use Build and push an amd64 image: docker buildx build --platform linux/amd64 -t yourusername/imagename:tag . --push This process allows you to create images that can run on different CPU architectures, improving portability across various systems. Best Practices Use official base images when possible. Minimize the number of layers in your Dockerfile. Use multi-stage builds to reduce final image size. Regularly update your Docker images to include security patches. Use Docker Compose for managing multi-container applications. For more advanced usage and best practices, refer to the Docker documentation .","title":"Docker"},{"location":"software/Docker/#docker","text":"Docker is a platform for developing, shipping, and running applications in containers. It enables developers to package an application with all of its dependencies into a standardized unit for software development. Docker containers are lightweight and contain everything needed to run the application, including code, runtime, system tools, system libraries, and settings.","title":"Docker"},{"location":"software/Docker/#docker-installations","text":"There are two main Docker installations: Docker Desktop for Linux Docker Engine Docker Desktop is recommended for beginners as it provides a user-friendly interface and additional features. Source","title":"Docker Installations"},{"location":"software/Docker/#installation","text":"Caution Always check the official Docker documentation for the most up-to-date instructions. The following steps are for Ubuntu 22.04.","title":"Installation"},{"location":"software/Docker/#preparing-for-installation","text":"First, remove any old or unofficial versions of Docker: for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done","title":"Preparing for Installation"},{"location":"software/Docker/#ubuntu-installation","text":"Set up Docker's apt repository: Follow the instructions in the official guide . Install Docker Desktop: Follow the steps outlined here . Note You may need to enable virtualization in your system's BIOS settings for Docker Desktop to work properly.","title":"Ubuntu Installation"},{"location":"software/Docker/#setting-up-pass-for-credential-storage","text":"Docker uses pass to store credentials securely. Initialize it with: gpg --generate-key pass init \"Your GPG ID or email\"","title":"Setting up pass for Credential Storage"},{"location":"software/Docker/#installing-rstudio-server-via-docker","text":"To run RStudio Server in a Docker container: Pull the RStudio Server image: docker pull rocker/rstudio Run the container: docker run -d -p 8787:8787 -e PASSWORD=yourpassword --name rstudio rocker/rstudio This command: Runs the container in detached mode ( -d ) Maps port 8787 on the host to port 8787 in the container ( -p 8787:8787 ) Sets a password for the RStudio user ( -e PASSWORD=yourpassword ) Names the container \"rstudio\" ( --name rstudio ) Access RStudio Server by navigating to http://localhost:8787 in your web browser.","title":"Installing RStudio Server via Docker"},{"location":"software/Docker/#docker-engine-examples","text":"","title":"Docker Engine Examples"},{"location":"software/Docker/#building-multi-architecture-images","text":"To build images for different architectures: Set up buildx: docker buildx create --use Build and push an amd64 image: docker buildx build --platform linux/amd64 -t yourusername/imagename:tag . --push This process allows you to create images that can run on different CPU architectures, improving portability across various systems.","title":"Building Multi-Architecture Images"},{"location":"software/Docker/#best-practices","text":"Use official base images when possible. Minimize the number of layers in your Dockerfile. Use multi-stage builds to reduce final image size. Regularly update your Docker images to include security patches. Use Docker Compose for managing multi-container applications. For more advanced usage and best practices, refer to the Docker documentation .","title":"Best Practices"},{"location":"software/R/","text":"R, RStudio, and RStudio Server Overview R is a programming language and environment for statistical graphics. RStudio is an integrated development environment (IDE) for R, providing a console, syntax-highlighting editor, and tools for plotting, debugging, and workspace management. RStudio Server is a version of RStudio that runs on a remote server and is accessed through a web browser, ideal for cloud-based or shared computing environments. Installation Use the docker image rocker/rstudio to run RStudio Server in a containerized environment. You can pull the image from Docker Hub and run it with the following command: docker run -d -p 8787:8787 -e PASSWORD=yourpasswordhere rocker/rstudio Docker containers for Bioconductor Alternatively, you can use Bioconductor's Docker images, docker run \\ -e PASSWORD=bioc \\ -p 8787:8787 \\ bioconductor/bioconductor_docker:devel Read the docs for more information. Bioconductor bioc-run script For your convenience, you can use the bioc-run script to run Bioconductor without having to remember the docker command. Download and install it with: mkdir -p ~/.local/bin curl https://raw.githubusercontent.com/Bioconductor/bioc-run/refs/heads/devel/bioc-run > ~/.local/bin/bioc-run chmod +x ~/.local/bin/bioc-run echo 'export PATH=$PATH:~/.local/bin' >> ~/.bashrc exec \"$SHELL\" Note Prerequisites: bash and jq If you use another shell like zsh , replace ~/.bashrc with ~/.zshrc , or another appropriate configuration file for your shell. Connecting to RStudio Server Find your EC2 instance's public IP address in the AWS console. Ensure that inbound traffic on port 8787 is allowed in the EC2 security group settings. Access RStudio Server by navigating to http://ec2-public-ip:8787 in your web browser. Note RStudio Server uses the underlying Linux system's user accounts. If not using a docker conntainer, the default ubuntu user doesn't have a password set, so create one frist with sudo passwd ubuntu . Package Management Installing Packages For system-wide package installation, ensure you have the necessary build tools: sudo apt update && sudo apt install build-essential To install specific versions of R packages: install.packages(\"remotes\") remotes::install_version(\"Seurat\", version = \"4.4.1\") renv: R Environment Management renv is a package management tool for R that creates project-specific libraries, ensuring reproducibility across different environments. Setting up renv: install.packages(\"renv\") renv::init() Using renv: install.packages(\"dplyr\") # Install packages as usual renv::snapshot() # Snapshot your project's state renv::restore() # Restore your project's state on another machine Best practices with renv: Commit renv.lock to version control. Use renv::status() to check for unsaved changes in your project's package list. Regularly update your snapshot with renv::snapshot() . RStudio provides visual cues for renv usage in the \"Packages\" pane and access to common renv functions through the \"renv\" menu. Troubleshooting If renv::restore() fails, check for package conflicts and ensure necessary system libraries are installed. If sessionInfo() shows unexpected packages, check your .Rprofile for auto-loaded packages and verify you're in the correct project environment. Interesting Facts and Advanced Topics R's origins: R was created by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand, and is named after the first letter of its creators' names. Source: R-project.org RStudio's impact: RStudio (now Posit) has significantly contributed to R's popularity. The RStudio IDE is used by an estimated 2.5 million people worldwide. Source: Posit R in research: R is widely used in academic research. A study found that R was mentioned in over 400,000 scholarly articles between 2011-2020. Source: IEEE Spectrum Parallel computing in R: You can leverage multi-core processors for faster computations using packages like parallel and foreach . Source: CRAN Task View R Markdown: This powerful tool allows you to create dynamic documents that combine code, results, and narrative text. It's excellent for reproducible research. Source: R Markdown book Shiny: A web application framework for R that allows you to create interactive web apps directly from R. It's widely used for creating dashboards and data exploration tools. Source: Shiny by Posit Configuring RStudio Server for multiple users: You can set up RStudio Server to support multiple users, each with their own workspace and permissions. Source: RStudio Server Admin Guide Setting up RStudio Server with SSL: For secure connections, especially important in cloud environments, you can configure RStudio Server to use SSL. Source: RStudio Server Secure Socket Layer (SSL) Guide Integrating RStudio Server with version control: RStudio Server can be integrated with Git and SVN for version control, enhancing collaboration and code management. Source: RStudio Support - Version Control R Consortium: The R Consortium, supported by the Linux Foundation, funds development of R-related projects and organizes conferences. It includes major tech companies as members. Source: R Consortium For more detailed information, refer to the official RStudio documentation and renv documentation .","title":"R, RStudio, and RStudio Server"},{"location":"software/R/#r-rstudio-and-rstudio-server","text":"","title":"R, RStudio, and RStudio Server"},{"location":"software/R/#overview","text":"R is a programming language and environment for statistical graphics. RStudio is an integrated development environment (IDE) for R, providing a console, syntax-highlighting editor, and tools for plotting, debugging, and workspace management. RStudio Server is a version of RStudio that runs on a remote server and is accessed through a web browser, ideal for cloud-based or shared computing environments.","title":"Overview"},{"location":"software/R/#installation","text":"Use the docker image rocker/rstudio to run RStudio Server in a containerized environment. You can pull the image from Docker Hub and run it with the following command: docker run -d -p 8787:8787 -e PASSWORD=yourpasswordhere rocker/rstudio","title":"Installation"},{"location":"software/R/#docker-containers-for-bioconductor","text":"Alternatively, you can use Bioconductor's Docker images, docker run \\ -e PASSWORD=bioc \\ -p 8787:8787 \\ bioconductor/bioconductor_docker:devel Read the docs for more information.","title":"Docker containers for Bioconductor"},{"location":"software/R/#bioconductor-bioc-run-script","text":"For your convenience, you can use the bioc-run script to run Bioconductor without having to remember the docker command. Download and install it with: mkdir -p ~/.local/bin curl https://raw.githubusercontent.com/Bioconductor/bioc-run/refs/heads/devel/bioc-run > ~/.local/bin/bioc-run chmod +x ~/.local/bin/bioc-run echo 'export PATH=$PATH:~/.local/bin' >> ~/.bashrc exec \"$SHELL\" Note Prerequisites: bash and jq If you use another shell like zsh , replace ~/.bashrc with ~/.zshrc , or another appropriate configuration file for your shell.","title":"Bioconductor bioc-run script"},{"location":"software/R/#connecting-to-rstudio-server","text":"Find your EC2 instance's public IP address in the AWS console. Ensure that inbound traffic on port 8787 is allowed in the EC2 security group settings. Access RStudio Server by navigating to http://ec2-public-ip:8787 in your web browser. Note RStudio Server uses the underlying Linux system's user accounts. If not using a docker conntainer, the default ubuntu user doesn't have a password set, so create one frist with sudo passwd ubuntu .","title":"Connecting to RStudio Server"},{"location":"software/R/#package-management","text":"","title":"Package Management"},{"location":"software/R/#installing-packages","text":"For system-wide package installation, ensure you have the necessary build tools: sudo apt update && sudo apt install build-essential To install specific versions of R packages: install.packages(\"remotes\") remotes::install_version(\"Seurat\", version = \"4.4.1\")","title":"Installing Packages"},{"location":"software/R/#renv-r-environment-management","text":"renv is a package management tool for R that creates project-specific libraries, ensuring reproducibility across different environments. Setting up renv: install.packages(\"renv\") renv::init() Using renv: install.packages(\"dplyr\") # Install packages as usual renv::snapshot() # Snapshot your project's state renv::restore() # Restore your project's state on another machine Best practices with renv: Commit renv.lock to version control. Use renv::status() to check for unsaved changes in your project's package list. Regularly update your snapshot with renv::snapshot() . RStudio provides visual cues for renv usage in the \"Packages\" pane and access to common renv functions through the \"renv\" menu.","title":"renv: R Environment Management"},{"location":"software/R/#troubleshooting","text":"If renv::restore() fails, check for package conflicts and ensure necessary system libraries are installed. If sessionInfo() shows unexpected packages, check your .Rprofile for auto-loaded packages and verify you're in the correct project environment.","title":"Troubleshooting"},{"location":"software/R/#interesting-facts-and-advanced-topics","text":"R's origins: R was created by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand, and is named after the first letter of its creators' names. Source: R-project.org RStudio's impact: RStudio (now Posit) has significantly contributed to R's popularity. The RStudio IDE is used by an estimated 2.5 million people worldwide. Source: Posit R in research: R is widely used in academic research. A study found that R was mentioned in over 400,000 scholarly articles between 2011-2020. Source: IEEE Spectrum Parallel computing in R: You can leverage multi-core processors for faster computations using packages like parallel and foreach . Source: CRAN Task View R Markdown: This powerful tool allows you to create dynamic documents that combine code, results, and narrative text. It's excellent for reproducible research. Source: R Markdown book Shiny: A web application framework for R that allows you to create interactive web apps directly from R. It's widely used for creating dashboards and data exploration tools. Source: Shiny by Posit Configuring RStudio Server for multiple users: You can set up RStudio Server to support multiple users, each with their own workspace and permissions. Source: RStudio Server Admin Guide Setting up RStudio Server with SSL: For secure connections, especially important in cloud environments, you can configure RStudio Server to use SSL. Source: RStudio Server Secure Socket Layer (SSL) Guide Integrating RStudio Server with version control: RStudio Server can be integrated with Git and SVN for version control, enhancing collaboration and code management. Source: RStudio Support - Version Control R Consortium: The R Consortium, supported by the Linux Foundation, funds development of R-related projects and organizes conferences. It includes major tech companies as members. Source: R Consortium For more detailed information, refer to the official RStudio documentation and renv documentation .","title":"Interesting Facts and Advanced Topics"},{"location":"software/bioconda/","text":"Bioconda and Micromamba Overview Bioconda : A channel for the conda package manager specializing in bioinformatics software. Micromamba : A fast, lightweight implementation of the conda package manager. Bioconda Channels Bioconda traditionally recommends setting up channels as follows: conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge conda config --set channel_priority strict This creates a .condarc file in the user's home directory: channels: - defaults - bioconda - conda-forge channel_priority: strict However, the Mamba project (which includes Micromamba) suggests a different approach. They recommend avoiding the Anaconda default channels due to potential conflicts and slower resolution times. Instead, they advocate for using only the necessary channels for each project. Micromamba: A Lightweight Alternative Micromamba is a smaller, faster version of the conda package manager. It's particularly useful for bioinformatics workflows due to its speed and reduced resource requirements. Why Use Micromamba? Faster package resolution and installation Smaller disk footprint No base environment, reducing conflicts Suitable for CI/CD pipelines and containers Environment Management with Micromamba Best practice is to use environment specification files for each project: name: RNAseq channels: - bioconda - conda-forge dependencies: - fastqc=0.11.9 - hisat2=2.2.1 - bwa=0.7.17 - bowtie2=2.4.2 - samtools=1.13 - htslib=1.13 - bcftools=1.13 - stringtie=2.1.7 - subread=2.0.1 Save this as RNAseq.yaml , then create the environment: micromamba env create -f RNAseq.yaml Activate the environment: micromamba activate RNAseq Your prompt should now show the active environment: (RNAseq) user@host:~$ Best Practices Specify exact versions for reproducibility. Use separate environments for different projects or workflows. Regularly update your environment files as tools are updated. Include only necessary channels to avoid conflicts. Use micromamba env export > environment.yaml to save current environment state. Troubleshooting If package conflicts occur, try removing the problematic package and reinstalling. Use micromamba clean --all to clear package caches if you encounter strange behavior. If a package is not found, ensure you've included the correct channel. Advanced Usage Use micromamba run -n env_name command to run commands in a specific environment without activating it. Create environments on-the-fly with micromamba create -n myenv package1 package2 . For more information, refer to the Bioconda documentation and Micromamba documentation .","title":"Bioconda and Micromamba"},{"location":"software/bioconda/#bioconda-and-micromamba","text":"","title":"Bioconda and Micromamba"},{"location":"software/bioconda/#overview","text":"Bioconda : A channel for the conda package manager specializing in bioinformatics software. Micromamba : A fast, lightweight implementation of the conda package manager.","title":"Overview"},{"location":"software/bioconda/#bioconda-channels","text":"Bioconda traditionally recommends setting up channels as follows: conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge conda config --set channel_priority strict This creates a .condarc file in the user's home directory: channels: - defaults - bioconda - conda-forge channel_priority: strict However, the Mamba project (which includes Micromamba) suggests a different approach. They recommend avoiding the Anaconda default channels due to potential conflicts and slower resolution times. Instead, they advocate for using only the necessary channels for each project.","title":"Bioconda Channels"},{"location":"software/bioconda/#micromamba-a-lightweight-alternative","text":"Micromamba is a smaller, faster version of the conda package manager. It's particularly useful for bioinformatics workflows due to its speed and reduced resource requirements.","title":"Micromamba: A Lightweight Alternative"},{"location":"software/bioconda/#why-use-micromamba","text":"Faster package resolution and installation Smaller disk footprint No base environment, reducing conflicts Suitable for CI/CD pipelines and containers","title":"Why Use Micromamba?"},{"location":"software/bioconda/#environment-management-with-micromamba","text":"Best practice is to use environment specification files for each project: name: RNAseq channels: - bioconda - conda-forge dependencies: - fastqc=0.11.9 - hisat2=2.2.1 - bwa=0.7.17 - bowtie2=2.4.2 - samtools=1.13 - htslib=1.13 - bcftools=1.13 - stringtie=2.1.7 - subread=2.0.1 Save this as RNAseq.yaml , then create the environment: micromamba env create -f RNAseq.yaml Activate the environment: micromamba activate RNAseq Your prompt should now show the active environment: (RNAseq) user@host:~$","title":"Environment Management with Micromamba"},{"location":"software/bioconda/#best-practices","text":"Specify exact versions for reproducibility. Use separate environments for different projects or workflows. Regularly update your environment files as tools are updated. Include only necessary channels to avoid conflicts. Use micromamba env export > environment.yaml to save current environment state.","title":"Best Practices"},{"location":"software/bioconda/#troubleshooting","text":"If package conflicts occur, try removing the problematic package and reinstalling. Use micromamba clean --all to clear package caches if you encounter strange behavior. If a package is not found, ensure you've included the correct channel.","title":"Troubleshooting"},{"location":"software/bioconda/#advanced-usage","text":"Use micromamba run -n env_name command to run commands in a specific environment without activating it. Create environments on-the-fly with micromamba create -n myenv package1 package2 . For more information, refer to the Bioconda documentation and Micromamba documentation .","title":"Advanced Usage"},{"location":"software/jupyter/","text":"Jupyter Jupyter notebooks provide an interactive environment for data analysis, visualization, and code execution. They are particularly useful for bioinformatics workflows, exploratory data analysis, and sharing reproducible research. Setting Up Jupyter on an EC2 Instance When running Jupyter on an EC2 instance, you need to configure it to allow remote access securely. Follow these steps: Install Jupyter (if not already installed): pip install jupyter Generate a config file: jupyter notebook --generate-config Create a password for added security: jupyter notebook password Edit the config file: nano ~/.jupyter/jupyter_notebook_config.py Add or modify these lines in the config file: c.NotebookApp.ip = '0.0.0.0' c.NotebookApp.open_browser = False c.NotebookApp.port = 8888 c.NotebookApp.allow_origin = '*' c.NotebookApp.allow_remote_access = True Start Jupyter: jupyter notebook --no-browser Accessing Jupyter from Your Local Machine Method 1: Direct Access (Less Secure) Ensure port 8888 is open in your EC2 security group. Access Jupyter using your EC2 instance's public IP: https://<your-ec2-public-ip>:8888 Enter the password you set earlier. Method 2: SSH Tunnel (More Secure, Recommended) Create an SSH tunnel: ssh -i <your-key.pem> -L 8888:localhost:8888 ubuntu@<your-ec2-public-ip> Access Jupyter locally: http://localhost:8888 Enter the password you set earlier. Best Practices Use virtual environments to manage dependencies: python -m venv jupyter_env source jupyter_env/bin/activate pip install jupyter Install and use JupyterLab for a more feature-rich environment: pip install jupyterlab jupyter lab --no-browser Use version control for notebooks. Install nbstripout to remove output before committing: pip install nbstripout nbstripout --install For long-running tasks, use tmux or screen : tmux new -s jupyter_session jupyter notebook --no-browser # Press Ctrl-B then D to detach # To reattach: tmux attach -t jupyter_session Regularly update Jupyter and its dependencies: pip install --upgrade jupyter jupyterlab Troubleshooting If you're having trouble connecting to Jupyter, check the following: EC2 Security Group: Ensure port 8888 is open (for direct access method). Jupyter Configuration: Verify the config file settings. Firewall: Check if the EC2 instance's firewall is blocking connections. Jupyter Server: Confirm Jupyter is running and note any error messages. SSL Certificate: If using HTTPS, ensure your certificate is valid and trusted. Advanced Topics Setting up JupyterHub for multi-user environments Integrating Jupyter with AWS S3 for data storage Using Jupyter kernels for different programming languages (R, Julia, etc.) Remember to always prioritize security when working with remote servers and sensitive data. Regularly update your EC2 instance, Jupyter, and all dependencies to ensure you have the latest security patches.","title":"Jupyter"},{"location":"software/jupyter/#jupyter","text":"Jupyter notebooks provide an interactive environment for data analysis, visualization, and code execution. They are particularly useful for bioinformatics workflows, exploratory data analysis, and sharing reproducible research.","title":"Jupyter"},{"location":"software/jupyter/#setting-up-jupyter-on-an-ec2-instance","text":"When running Jupyter on an EC2 instance, you need to configure it to allow remote access securely. Follow these steps: Install Jupyter (if not already installed): pip install jupyter Generate a config file: jupyter notebook --generate-config Create a password for added security: jupyter notebook password Edit the config file: nano ~/.jupyter/jupyter_notebook_config.py Add or modify these lines in the config file: c.NotebookApp.ip = '0.0.0.0' c.NotebookApp.open_browser = False c.NotebookApp.port = 8888 c.NotebookApp.allow_origin = '*' c.NotebookApp.allow_remote_access = True Start Jupyter: jupyter notebook --no-browser","title":"Setting Up Jupyter on an EC2 Instance"},{"location":"software/jupyter/#accessing-jupyter-from-your-local-machine","text":"","title":"Accessing Jupyter from Your Local Machine"},{"location":"software/jupyter/#method-1-direct-access-less-secure","text":"Ensure port 8888 is open in your EC2 security group. Access Jupyter using your EC2 instance's public IP: https://<your-ec2-public-ip>:8888 Enter the password you set earlier.","title":"Method 1: Direct Access (Less Secure)"},{"location":"software/jupyter/#method-2-ssh-tunnel-more-secure-recommended","text":"Create an SSH tunnel: ssh -i <your-key.pem> -L 8888:localhost:8888 ubuntu@<your-ec2-public-ip> Access Jupyter locally: http://localhost:8888 Enter the password you set earlier.","title":"Method 2: SSH Tunnel (More Secure, Recommended)"},{"location":"software/jupyter/#best-practices","text":"Use virtual environments to manage dependencies: python -m venv jupyter_env source jupyter_env/bin/activate pip install jupyter Install and use JupyterLab for a more feature-rich environment: pip install jupyterlab jupyter lab --no-browser Use version control for notebooks. Install nbstripout to remove output before committing: pip install nbstripout nbstripout --install For long-running tasks, use tmux or screen : tmux new -s jupyter_session jupyter notebook --no-browser # Press Ctrl-B then D to detach # To reattach: tmux attach -t jupyter_session Regularly update Jupyter and its dependencies: pip install --upgrade jupyter jupyterlab","title":"Best Practices"},{"location":"software/jupyter/#troubleshooting","text":"If you're having trouble connecting to Jupyter, check the following: EC2 Security Group: Ensure port 8888 is open (for direct access method). Jupyter Configuration: Verify the config file settings. Firewall: Check if the EC2 instance's firewall is blocking connections. Jupyter Server: Confirm Jupyter is running and note any error messages. SSL Certificate: If using HTTPS, ensure your certificate is valid and trusted.","title":"Troubleshooting"},{"location":"software/jupyter/#advanced-topics","text":"Setting up JupyterHub for multi-user environments Integrating Jupyter with AWS S3 for data storage Using Jupyter kernels for different programming languages (R, Julia, etc.) Remember to always prioritize security when working with remote servers and sensitive data. Regularly update your EC2 instance, Jupyter, and all dependencies to ensure you have the latest security patches.","title":"Advanced Topics"},{"location":"software/samtools/","text":"Samtools Samtools is a suite of programs for manipulating high-throughput sequencing data consisting of: Samtools : For handling SAM/BAM/CRAM formats BCFtools : Variant calling (VCF/BCF formats) HTSlib : The underlying C library Tip Read the official samtools documentation here samtools Commands Indexing Command Description Syntax dict create a sequence dictionary file faidx index/extract FASTA fqidx index/extract FASTQ index index alignment Editing Command Description Syntax calmd recalculate MD/NM tags and '=' bases fixmate fix mate information reheader replace BAM header targetcut cut fosmid regions (for fosmid pool only) addreplacerg adds or replaces RG tags markdup mark duplicates samtools markdup in.algnsorted.bam out.bam ampliconclip clip oligos from the end of reads Handling Duplicate Reads Let's assume we already have sorted, marked duplicates, and indexed BAM files. for bamfile in ./*.bam; do # note that in this case, files are named like `DNMT3A_sorted_markdup.bam` samtools markdup @$(nproc) -r -s $bamfile ${bamfile%markdup.bam}dedup.bam done File operations Command Description Syntax collate shuffle and group alignments by name cat concatenate BAMs consensus produce a consensus Pileup/FASTA/FASTQ merge merge sorted alignments mpileup multi-way pileup sort sort alignment file split splits a file by read group quickcheck quickly check if SAM/BAM/CRAM file appears intact fastq converts a BAM to a FASTQ fasta converts a BAM to a FASTA import Converts FASTA or FASTQ files to SAM/BAM/CRAM reference Generates a reference from aligned data reset Reverts aligner changes in reads Sorting and Indexing samtools sort : Sort alignments by leftmost coordinates Sort a BAM file: samtools sort -o sorted_output.bam input.bam Sort by read names (useful for some tools): samtools sort -n -o name_sorted.bam input.bam samtools index : Index a sorted BAM/CRAM file samtools index sorted_output.bam Tip Always index your sorted BAM files. Many tools require indexed BAMs for efficient access. Merging and Splitting samtools merge : Combine multiple BAM files samtools merge output.bam input1.bam input2.bam input3.bam Ensure all input BAMs are sorted in the same way (coordinate or query name). samtools split : Split a BAM file by read group samtools split -u unassigned.bam -f '%*_%!.bam' input.bam This is useful when you have multiple samples in one BAM file. Statistics Command Description Syntax bedcov read depth per BED region coverage alignment depth and percent coverage depth compute the depth flagstat simple stats idxstats BAM index stats cram-size list CRAM Content-ID and Data-Series sizes phase phase heterozygotes stats generate stats (former bamcheck) ampliconstats generate amplicon specific stats samtools flagstat : Generate simple alignment statistics samtools flagstat input.bam samtools idxstats : Report alignment summary statistics samtools idxstats input.bam samtools stats : Generate comprehensive statistics samtools stats input.bam > stats.txt Use plot-bamstats to visualize these statistics. Viewing Command Description Syntax flags explain BAM flags head header viewer tview text alignment viewer view SAM<->BAM<->CRAM conversion depad convert padded BAM to unpadded BAM samples list the samples in a set of SAM/BAM/CRAM files Convert SAM to BAM: samtools view -b -o output.bam input.sam Convert BAM to CRAM (requires indexed reference genome): samtools view -C -T reference.fa -o output.cram input.bam View specific region of a BAM file: samtools view input.bam chr1:1000000-2000000 Filter for mapped reads and convert to SAM: samtools view -F 4 -h -o mapped_reads.sam input.bam Note The -F 4 flag filters out unmapped reads. Run samtools flags for more info. Working with CRAM Files CRAM is a compressed alternative to BAM, offering significant space savings. Converting BAM to CRAM: samtools view -C -T reference.fa -o output.cram input.bam Important Considerations for CRAM Always keep your reference genome available. Use the exact same reference for creating and reading CRAM files. Set the REF_PATH environment variable to help samtools locate reference sequences: export REF_PATH=\"/path/to/references/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s\" This allows for local and EBI-hosted reference lookup. Best Practices and Tips Always work with sorted and indexed BAM/CRAM files for efficiency. Use multithreading when available: samtools sort -@ 4 -o sorted.bam input.bam When working with large files, use the -c option to compress temporary files: samtools sort -c -m 4G -o sorted.bam input.bam For variant calling workflows, mark duplicates: samtools markdup input.bam output.bam Use samtools faidx to index your reference genome: samtools faidx reference.fa When downloading reference genomes, ensure they're bgzip-compressed: # If you have a gzipped file: gunzip reference.fa.gz bgzip reference.fa samtools faidx reference.fa.gz For large-scale analyses, consider using CRAM format to save storage space. Practical Workflow Example Here's a typical workflow for processing a new sequencing run: # Convert FASTQ to BAM (assuming you've already aligned with BWA) bwa mem -t 8 reference.fa read1.fq read2.fq | samtools view -b -o raw_aligned.bam - # Sort the BAM file samtools sort -@ 8 -o sorted.bam raw_aligned.bam # Mark duplicates samtools markdup -@ 8 sorted.bam marked_duplicates.bam # Index the final BAM samtools index marked_duplicates.bam # Generate alignment statistics samtools flagstat marked_duplicates.bam > alignment_stats.txt samtools idxstats marked_duplicates.bam > chromosome_stats.txt # Convert to CRAM for storage samtools view -C -T reference.fa -o final_output.cram marked_duplicates.bam Example Pipelines","title":"Samtools"},{"location":"software/samtools/#samtools","text":"Samtools is a suite of programs for manipulating high-throughput sequencing data consisting of: Samtools : For handling SAM/BAM/CRAM formats BCFtools : Variant calling (VCF/BCF formats) HTSlib : The underlying C library Tip Read the official samtools documentation here","title":"Samtools"},{"location":"software/samtools/#samtools-commands","text":"","title":"samtools Commands"},{"location":"software/samtools/#indexing","text":"Command Description Syntax dict create a sequence dictionary file faidx index/extract FASTA fqidx index/extract FASTQ index index alignment","title":"Indexing"},{"location":"software/samtools/#editing","text":"Command Description Syntax calmd recalculate MD/NM tags and '=' bases fixmate fix mate information reheader replace BAM header targetcut cut fosmid regions (for fosmid pool only) addreplacerg adds or replaces RG tags markdup mark duplicates samtools markdup in.algnsorted.bam out.bam ampliconclip clip oligos from the end of reads","title":"Editing"},{"location":"software/samtools/#handling-duplicate-reads","text":"Let's assume we already have sorted, marked duplicates, and indexed BAM files. for bamfile in ./*.bam; do # note that in this case, files are named like `DNMT3A_sorted_markdup.bam` samtools markdup @$(nproc) -r -s $bamfile ${bamfile%markdup.bam}dedup.bam done","title":"Handling Duplicate Reads"},{"location":"software/samtools/#file-operations","text":"Command Description Syntax collate shuffle and group alignments by name cat concatenate BAMs consensus produce a consensus Pileup/FASTA/FASTQ merge merge sorted alignments mpileup multi-way pileup sort sort alignment file split splits a file by read group quickcheck quickly check if SAM/BAM/CRAM file appears intact fastq converts a BAM to a FASTQ fasta converts a BAM to a FASTA import Converts FASTA or FASTQ files to SAM/BAM/CRAM reference Generates a reference from aligned data reset Reverts aligner changes in reads","title":"File operations"},{"location":"software/samtools/#sorting-and-indexing","text":"samtools sort : Sort alignments by leftmost coordinates Sort a BAM file: samtools sort -o sorted_output.bam input.bam Sort by read names (useful for some tools): samtools sort -n -o name_sorted.bam input.bam samtools index : Index a sorted BAM/CRAM file samtools index sorted_output.bam Tip Always index your sorted BAM files. Many tools require indexed BAMs for efficient access.","title":"Sorting and Indexing"},{"location":"software/samtools/#merging-and-splitting","text":"samtools merge : Combine multiple BAM files samtools merge output.bam input1.bam input2.bam input3.bam Ensure all input BAMs are sorted in the same way (coordinate or query name). samtools split : Split a BAM file by read group samtools split -u unassigned.bam -f '%*_%!.bam' input.bam This is useful when you have multiple samples in one BAM file.","title":"Merging and Splitting"},{"location":"software/samtools/#statistics","text":"Command Description Syntax bedcov read depth per BED region coverage alignment depth and percent coverage depth compute the depth flagstat simple stats idxstats BAM index stats cram-size list CRAM Content-ID and Data-Series sizes phase phase heterozygotes stats generate stats (former bamcheck) ampliconstats generate amplicon specific stats samtools flagstat : Generate simple alignment statistics samtools flagstat input.bam samtools idxstats : Report alignment summary statistics samtools idxstats input.bam samtools stats : Generate comprehensive statistics samtools stats input.bam > stats.txt Use plot-bamstats to visualize these statistics.","title":"Statistics"},{"location":"software/samtools/#viewing","text":"Command Description Syntax flags explain BAM flags head header viewer tview text alignment viewer view SAM<->BAM<->CRAM conversion depad convert padded BAM to unpadded BAM samples list the samples in a set of SAM/BAM/CRAM files Convert SAM to BAM: samtools view -b -o output.bam input.sam Convert BAM to CRAM (requires indexed reference genome): samtools view -C -T reference.fa -o output.cram input.bam View specific region of a BAM file: samtools view input.bam chr1:1000000-2000000 Filter for mapped reads and convert to SAM: samtools view -F 4 -h -o mapped_reads.sam input.bam Note The -F 4 flag filters out unmapped reads. Run samtools flags for more info.","title":"Viewing"},{"location":"software/samtools/#working-with-cram-files","text":"CRAM is a compressed alternative to BAM, offering significant space savings. Converting BAM to CRAM: samtools view -C -T reference.fa -o output.cram input.bam","title":"Working with CRAM Files"},{"location":"software/samtools/#important-considerations-for-cram","text":"Always keep your reference genome available. Use the exact same reference for creating and reading CRAM files. Set the REF_PATH environment variable to help samtools locate reference sequences: export REF_PATH=\"/path/to/references/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s\" This allows for local and EBI-hosted reference lookup.","title":"Important Considerations for CRAM"},{"location":"software/samtools/#best-practices-and-tips","text":"Always work with sorted and indexed BAM/CRAM files for efficiency. Use multithreading when available: samtools sort -@ 4 -o sorted.bam input.bam When working with large files, use the -c option to compress temporary files: samtools sort -c -m 4G -o sorted.bam input.bam For variant calling workflows, mark duplicates: samtools markdup input.bam output.bam Use samtools faidx to index your reference genome: samtools faidx reference.fa When downloading reference genomes, ensure they're bgzip-compressed: # If you have a gzipped file: gunzip reference.fa.gz bgzip reference.fa samtools faidx reference.fa.gz For large-scale analyses, consider using CRAM format to save storage space.","title":"Best Practices and Tips"},{"location":"software/samtools/#practical-workflow-example","text":"Here's a typical workflow for processing a new sequencing run: # Convert FASTQ to BAM (assuming you've already aligned with BWA) bwa mem -t 8 reference.fa read1.fq read2.fq | samtools view -b -o raw_aligned.bam - # Sort the BAM file samtools sort -@ 8 -o sorted.bam raw_aligned.bam # Mark duplicates samtools markdup -@ 8 sorted.bam marked_duplicates.bam # Index the final BAM samtools index marked_duplicates.bam # Generate alignment statistics samtools flagstat marked_duplicates.bam > alignment_stats.txt samtools idxstats marked_duplicates.bam > chromosome_stats.txt # Convert to CRAM for storage samtools view -C -T reference.fa -o final_output.cram marked_duplicates.bam","title":"Practical Workflow Example"},{"location":"software/samtools/#example-pipelines","text":"","title":"Example Pipelines"},{"location":"workflows/","text":"Illumina Sequencing If you used a core facility, Azenta, or another commercial service for sequencing, they will send link to directly download the de-multiplexed FASTQ files, usually with corresponding md5 checksums. For instructions on how to download data from Azenta's sFTP server, click here . Note sFTP (Secure File Transfer Protocol) provides an encrypted channel for data transfer.\\ The md5 checksum is a unique character sequence that is computed from the contents of a file and changes if the file is modified. Read the original RFC 1321 . Otherwise, consult the documentation for the appropriate Illumina sequencer: MiSeq NextSeq500 BaseSpace and the bs command-line interface The browser-based interface is useful for small-scale projects, but the command-line interface is more efficient for large-scale projects. Check out examples . Install on macOS using Homebrew: brew tap basespace/basespace && brew install bs-cli otherwise, download the latest version from Illumina: install_directory=\"${HOME}/.local/bin}\" basespace_executable=\"$install_directory/bs\" wget \"https://launch.basespace.illumina.com/CLI/latest/amd64-linux/bs\" -O \"$basespace_executable\" chmod +x \"$basespace_executable\" # add to PATH # echo \"export PATH=$install_directory:$PATH\" >> ~/.bashrc # or add an alias # echo \"alias bs=$basespace_executable\" >> ~/.bashrc After installation, authenticate with your BaseSpace credentials: bs authenticate # Follow link and login with illumina credentials, then run 'bs whoami' to verify that you are authenticated: +----------------+----------------------------------------------------+ | Name | Ryan Najac | | Id | ######## | | Email | rdn2108@cumc.columbia.edu | | DateCreated | 2021-07-13 15:29:51 +0000 UTC | | DateLastActive | 2024-06-03 18:59:47 +0000 UTC | | Host | https://api.basespace.illumina.com | | Scopes | READ GLOBAL, CREATE GLOBAL, BROWSE GLOBAL, | | | CREATE PROJECTS, CREATE RUNS, START APPLICATIONS, | | | MOVETOTRASH GLOBAL, WRITE GLOBAL | +----------------+----------------------------------------------------+ Demultiplexing Illumina sequencing data Illumina instruments will demultiplex the data for you if you provide a valid sample sheet prior to sequencing. For more information on how to create a sample sheet, consult the Illumina Support page. Skip ahead to the Quality Control section if the FASTQ files are already demultiplexed and ready for analysis, or keep reading for instructions on how to demultiplex the data yourself. Illumina hosts .rpm files for CentOS/RedHat Linux distros and the source code (which must be compiled) for other distros. Download bcl2fastq2 Conversion Software v2.20 Installer (Linux rpm) from Illumina . The AWS EC2 instance used for this project is based on Ubuntu, so we will have to convert the .rpm file to a .deb file using the alien package, as per this post . sudo alien -i bcl2fastq2-v2.20.0.422-Linux-x86_64.rpm Warning As of 2024-08-05, bcl2fastq is no longer supported; use bclconvert instead.\\ You can install bclconvert using the same methods as described above. Read the docs: bcl2fastq bclconvert Quality Control FASTQC mkdir -p ./fastqc for f in ./*.fastq.gz; do fastqc -o ./fastqc --noextract --memory 1024 \"$f\" & done; wait","title":"Illumina Sequencing"},{"location":"workflows/#illumina-sequencing","text":"If you used a core facility, Azenta, or another commercial service for sequencing, they will send link to directly download the de-multiplexed FASTQ files, usually with corresponding md5 checksums. For instructions on how to download data from Azenta's sFTP server, click here . Note sFTP (Secure File Transfer Protocol) provides an encrypted channel for data transfer.\\ The md5 checksum is a unique character sequence that is computed from the contents of a file and changes if the file is modified. Read the original RFC 1321 . Otherwise, consult the documentation for the appropriate Illumina sequencer: MiSeq NextSeq500","title":"Illumina Sequencing"},{"location":"workflows/#basespace-and-the-bs-command-line-interface","text":"The browser-based interface is useful for small-scale projects, but the command-line interface is more efficient for large-scale projects. Check out examples . Install on macOS using Homebrew: brew tap basespace/basespace && brew install bs-cli otherwise, download the latest version from Illumina: install_directory=\"${HOME}/.local/bin}\" basespace_executable=\"$install_directory/bs\" wget \"https://launch.basespace.illumina.com/CLI/latest/amd64-linux/bs\" -O \"$basespace_executable\" chmod +x \"$basespace_executable\" # add to PATH # echo \"export PATH=$install_directory:$PATH\" >> ~/.bashrc # or add an alias # echo \"alias bs=$basespace_executable\" >> ~/.bashrc After installation, authenticate with your BaseSpace credentials: bs authenticate # Follow link and login with illumina credentials, then run 'bs whoami' to verify that you are authenticated: +----------------+----------------------------------------------------+ | Name | Ryan Najac | | Id | ######## | | Email | rdn2108@cumc.columbia.edu | | DateCreated | 2021-07-13 15:29:51 +0000 UTC | | DateLastActive | 2024-06-03 18:59:47 +0000 UTC | | Host | https://api.basespace.illumina.com | | Scopes | READ GLOBAL, CREATE GLOBAL, BROWSE GLOBAL, | | | CREATE PROJECTS, CREATE RUNS, START APPLICATIONS, | | | MOVETOTRASH GLOBAL, WRITE GLOBAL | +----------------+----------------------------------------------------+","title":"BaseSpace and the bs command-line interface"},{"location":"workflows/#demultiplexing-illumina-sequencing-data","text":"Illumina instruments will demultiplex the data for you if you provide a valid sample sheet prior to sequencing. For more information on how to create a sample sheet, consult the Illumina Support page. Skip ahead to the Quality Control section if the FASTQ files are already demultiplexed and ready for analysis, or keep reading for instructions on how to demultiplex the data yourself. Illumina hosts .rpm files for CentOS/RedHat Linux distros and the source code (which must be compiled) for other distros. Download bcl2fastq2 Conversion Software v2.20 Installer (Linux rpm) from Illumina . The AWS EC2 instance used for this project is based on Ubuntu, so we will have to convert the .rpm file to a .deb file using the alien package, as per this post . sudo alien -i bcl2fastq2-v2.20.0.422-Linux-x86_64.rpm Warning As of 2024-08-05, bcl2fastq is no longer supported; use bclconvert instead.\\ You can install bclconvert using the same methods as described above. Read the docs: bcl2fastq bclconvert","title":"Demultiplexing Illumina sequencing data"},{"location":"workflows/#quality-control","text":"","title":"Quality Control"},{"location":"workflows/#fastqc","text":"mkdir -p ./fastqc for f in ./*.fastq.gz; do fastqc -o ./fastqc --noextract --memory 1024 \"$f\" & done; wait","title":"FASTQC"},{"location":"workflows/Azenta/","text":"Azenta sFTP Data Delivery Host : sftp://sftp.genewiz.com User : uni1234_cumc_columbia Password : #################### Port : 22 Login # Connects to sFTP and prompts for password sftp -P 22 $User@$Host Set Up Passwordless SSH Authentication Ensure your SSH public key is added to the authorized_keys file on the SFTP server. If you haven\u2019t done this yet, follow these steps: ssh-copy-id -i ~/.ssh/id_ed25519.pub user@remote_server Copying to an sftp server looks like: ssh-copy-id -i ~/.ssh/id_ed25519.pub hm2882_cumc_columbia","title":"Azenta sFTP Data Delivery"},{"location":"workflows/Azenta/#azenta-sftp-data-delivery","text":"Host : sftp://sftp.genewiz.com User : uni1234_cumc_columbia Password : #################### Port : 22","title":"Azenta sFTP Data Delivery"},{"location":"workflows/Azenta/#login","text":"# Connects to sFTP and prompts for password sftp -P 22 $User@$Host","title":"Login"},{"location":"workflows/Azenta/#set-up-passwordless-ssh-authentication","text":"Ensure your SSH public key is added to the authorized_keys file on the SFTP server. If you haven\u2019t done this yet, follow these steps: ssh-copy-id -i ~/.ssh/id_ed25519.pub user@remote_server Copying to an sftp server looks like: ssh-copy-id -i ~/.ssh/id_ed25519.pub hm2882_cumc_columbia","title":"Set Up Passwordless SSH Authentication"},{"location":"workflows/ChIP-Seq/","text":"ChIP-seq Analysis Overview Chromatin immunoprecipitation sequencing (ChIP-seq) is used to analyze protein interactions with DNA. It combines chromatin immunoprecipitation (ChIP) with massively parallel DNA sequencing to identify the binding sites of DNA-associated proteins. parse2wig parse2wig is a tool for converting aligned read files to wiggle format, which is useful for visualization in genome browsers. 3. Peak Calling Peak calling is a crucial step in ChIP-seq analysis, identifying regions of the genome where proteins of interest are bound. Several tools are available: Tool Description Source MACS2[^1] Model-based Analysis of ChIP-Seq. Captures local biases in sequencing data and models peak shape to improve accuracy. GitHub IDR[^2] Irreproducible Discovery Rate. Assesses reproducibility of findings across replicates. GitHub phantompeakqualtools[^3] A package for ChIP-seq quality control metrics, including strand cross-correlation. GitHub 4. Visualization and Analysis DROMPA DROMPA (DRaw and Observe Multiple enrichment Profiles and Annotation) is a comprehensive tool for easy-to-handle peak calling and visualization in ChIP-seq data analysis[^4]. Key features: Peak calling Visualization of enrichment profiles Annotation of peaks Computational analysis and validation of ChIP-seq data Feature Description Peak Calling Identifies significant binding sites using a sliding window approach Visualization Generates publication-quality figures of ChIP-seq enrichment profiles Annotation Associates peaks with genomic features (e.g., promoters, genes) Comparative Analysis Allows comparison of multiple ChIP-seq datasets References [^1]: Zhang Y, Liu T, Meyer CA, et al. Model-based Analysis of ChIP-Seq (MACS). Genome Biol. 2008;9(9):R137. PMID: 18798982 [^2]: Li Q, Brown JB, Huang H, Bickel PJ. Measuring reproducibility of high-throughput experiments. Ann Appl Stat. 2011;5(3):1752-1779. DOI: 10.1214/11-AOAS466 [^3]: Landt SG, Marinov GK, Kundaje A, et al. ChIP-seq guidelines and practices of the ENCODE and modENCODE consortia. Genome Res. 2012;22(9):1813-1831. PMID: 22955991 [^4]: Nakato R, Itoh T, Shirahige K. DROMPA: easy-to-handle peak calling and visualization software for the computational analysis and validation of ChIP-seq data. Genes Cells. 2013;18(7):589-601. PMID: 23672187","title":"ChIP-seq Analysis"},{"location":"workflows/ChIP-Seq/#chip-seq-analysis","text":"","title":"ChIP-seq Analysis"},{"location":"workflows/ChIP-Seq/#overview","text":"Chromatin immunoprecipitation sequencing (ChIP-seq) is used to analyze protein interactions with DNA. It combines chromatin immunoprecipitation (ChIP) with massively parallel DNA sequencing to identify the binding sites of DNA-associated proteins.","title":"Overview"},{"location":"workflows/ChIP-Seq/#parse2wig","text":"parse2wig is a tool for converting aligned read files to wiggle format, which is useful for visualization in genome browsers.","title":"parse2wig"},{"location":"workflows/ChIP-Seq/#3-peak-calling","text":"Peak calling is a crucial step in ChIP-seq analysis, identifying regions of the genome where proteins of interest are bound. Several tools are available: Tool Description Source MACS2[^1] Model-based Analysis of ChIP-Seq. Captures local biases in sequencing data and models peak shape to improve accuracy. GitHub IDR[^2] Irreproducible Discovery Rate. Assesses reproducibility of findings across replicates. GitHub phantompeakqualtools[^3] A package for ChIP-seq quality control metrics, including strand cross-correlation. GitHub","title":"3. Peak Calling"},{"location":"workflows/ChIP-Seq/#4-visualization-and-analysis","text":"","title":"4. Visualization and Analysis"},{"location":"workflows/ChIP-Seq/#drompa","text":"DROMPA (DRaw and Observe Multiple enrichment Profiles and Annotation) is a comprehensive tool for easy-to-handle peak calling and visualization in ChIP-seq data analysis[^4]. Key features: Peak calling Visualization of enrichment profiles Annotation of peaks Computational analysis and validation of ChIP-seq data Feature Description Peak Calling Identifies significant binding sites using a sliding window approach Visualization Generates publication-quality figures of ChIP-seq enrichment profiles Annotation Associates peaks with genomic features (e.g., promoters, genes) Comparative Analysis Allows comparison of multiple ChIP-seq datasets","title":"DROMPA"},{"location":"workflows/ChIP-Seq/#references","text":"[^1]: Zhang Y, Liu T, Meyer CA, et al. Model-based Analysis of ChIP-Seq (MACS). Genome Biol. 2008;9(9):R137. PMID: 18798982 [^2]: Li Q, Brown JB, Huang H, Bickel PJ. Measuring reproducibility of high-throughput experiments. Ann Appl Stat. 2011;5(3):1752-1779. DOI: 10.1214/11-AOAS466 [^3]: Landt SG, Marinov GK, Kundaje A, et al. ChIP-seq guidelines and practices of the ENCODE and modENCODE consortia. Genome Res. 2012;22(9):1813-1831. PMID: 22955991 [^4]: Nakato R, Itoh T, Shirahige K. DROMPA: easy-to-handle peak calling and visualization software for the computational analysis and validation of ChIP-seq data. Genes Cells. 2013;18(7):589-601. PMID: 23672187","title":"References"},{"location":"workflows/rnaseq/","text":"RNA Sequencing Analysis RNA sequencing (RNA-seq) is a technique used to analyze the transcriptome \u2014 the complete set of RNA transcripts in a cell. This method involves sequencing RNA molecules after reverse transcription to cDNA to examine gene expression levels and identify novel transcripts. Pre-requisites Before starting the analysis, ensure you have the following: Aligned reads in SAM/BAM/CRAM format A reference annotation file in GTF or GFF format A read quantification tool such as featureCounts or HTSeq This document describes two pipelines for RNA-seq analysis: Using featureCounts and then analyzing with DESeq2 in R Using the Tuxedo Suite (HISAT2, StringTie, Ballgown) featureCounts featureCounts is a program for counting reads mapped to genomic features, such as genes, exons, and promoters. It is part of the Subread package and can be used for RNA-seq as well as DNA-seq analysis. featureCounts takes as input SAM/BAM files and an annotation file including chromosomal coordinates of features. It outputs numbers of reads assigned to features (or meta-features). It also outputs stat info for the overall summrization results, including number of successfully assigned reads and number of reads that failed to be assigned due to various reasons (these reasons are included in the stat info). Use FeatureCounts to count the number of reads that map to each gene in a GTF file and summarize the results for downstream analysis (i.e., differential expression). Input Aligned reads in BAM format and a GTF file containing genomic features. Example usage: featureCounts -T \"$NUM_THREADS\" --verbose -t exon -g gene_id --countReadPairs \\ -a \"$REF_GTF\" -p -P -C -B -o \"${OUT_DIR}/${OUT_PREFIX}.tsv\" ./*.bam Or, more simply: featureCounts -a ${ANNOTATION} -p --countReadPairs -T$(nproc) -o total_counts.txt ./*.bam Annotation file from seqs_for_alignment_pipelines.ucsc_ids/ GCA_000001405.15_GRCh38_full_analysis_set.refseq_annotation.gff.gz 2024-09-10 13:15 74M GCA_000001405.15_GRCh38_full_analysis_set.refseq_annotation.gtf.gz Output Feature counts are written to a tab-delimited file ( .tsv or .txt ) with columns for each sample. You can import this file into R or other statistical software for further analysis. Feature counts also provide a summary of the number of reads that were assigned to features, as well as the number of reads that were not assigned to any feature. Downstream Analysis using R Export the results from featureCounts to R for further analysis, such as differential expression analysis using packages like DESeq2 or edgeR. Normalization Caution Without between sample normalization NONE of these units are comparable across experiments. This is a result of RNA-Seq being a relative measurement, not an absolute one. Source . Counts usually refers to the number of reads that align to a particular feature ($X_i). These numbers are heavily dependent on two things: The amount of fragments you sequenced (this is related to relative abundances). The length of the feature, or more appropriately, the effective length . Effective length refers to the number of possible start sites a feature could have generated a fragment of that particular length. Since counts are NOT scaled by the length of the feature, all units in this category are not comparable within a sample without adjusting for the feature length. This means you can\u2019t sum the counts over a set of features to get the expression of that set (e.g. you can\u2019t sum isoform counts to get gene counts). TPM (Transcripts Per Million) Transcripts per million (TPM) is a measurement of the proportion of transcripts in your pool of RNA. RPKM/FPKM Reads per kilobase of exon per million reads mapped (RPKM), or the more generic FPKM (substitute reads with fragments) are essentially the same thing. Note FPKM is not $2 *$ RPKM if you have paired-end reads. FPKM $==$ RPKM if you have single-end reads, and saying RPKM when you have paired-end reads is incorrect. Code example countToTpm <- function(counts, effLen) { rate <- log(counts) - log(effLen) denom <- log(sum(exp(rate))) exp(rate - denom + log(1e6)) } countToFpkm <- function(counts, effLen) { N <- sum(counts) exp( log(counts) + log(1e9) - log(effLen) - log(N) ) } fpkmToTpm <- function(fpkm) { exp(log(fpkm) - log(sum(fpkm)) + log(1e6)) } countToEffCounts <- function(counts, len, effLen) { counts * (len / effLen) } R Packages DESeq2 DESeq2 is an R package for differential gene expression analysis based on the negative binomial distribution. DESeq2 provides methods to test for differential expression by use of negative binomial generalized linear models. The models use the raw counts as input and perform regularized log transformation and variance stabilizing transformation. The package also provides functions to visualize the data and results. Tuxedo Suite The Tuxedo Suite is a collection of tools for transcript-level expression analysis of RNA-seq experiments. Align reads to the reference genome and sort to BAM format (HISAT2) Assemble transcripts (StringTie) Prepare for differential expression analysis (Ballgown setup) Perform differential expression analysis Visualize results Tool Description Manual Source HISAT2 Align reads to reference genome manual source StringTie Assemble RNA-Seq alignments into transcripts manual source Ballgown Isoform-level differential expression analysis manual source Tip StringTie comes packaged with gffcompare for comparing and evaluating the accuracy of RNA-seq transcript assemblers. Read the manual for details. Using Ballgown: Filter to remove low-abundance genes Identify differentially expressed transcripts and genes Add gene names Sort by p-value Write results to file References Liao Y, Smyth GK, Shi W. featureCounts: an efficient general purpose program for assigning sequence reads to genomic features. Bioinformatics. 2014;30(7):923-30. PMID: 24227677 Pertea, M., Kim, D., Pertea, G. M., Leek, J. T., & Salzberg, S. L. (2016). Transcript-level expression analysis of RNA-seq experiments with HISAT, StringTie and Ballgown. Nature Protocols, 11(9), 1650\u20131667. PMID: 27560171 Kim D, Langmead B, Salzberg SL. HISAT: a fast spliced aligner with low memory requirements. Nat Methods. 2015;12(4):357-360. PMID: 25751142 Pertea M, Pertea GM, Antonescu CM, Chang TC, Mendell JT, Salzberg SL. StringTie enables improved reconstruction of a transcriptome from RNA-seq reads. Nat Biotechnol. 2015;33(3):290-295. PMID: 25690850 Frazee AC, Pertea G, Jaffe AE, Langmead B, Salzberg SL, Leek JT. Ballgown bridges the gap between transcriptome assembly and expression analysis. Nat Biotechnol. 2015;33(3):243-246. PMID: 25748911","title":"RNA Sequencing Analysis"},{"location":"workflows/rnaseq/#rna-sequencing-analysis","text":"RNA sequencing (RNA-seq) is a technique used to analyze the transcriptome \u2014 the complete set of RNA transcripts in a cell. This method involves sequencing RNA molecules after reverse transcription to cDNA to examine gene expression levels and identify novel transcripts.","title":"RNA Sequencing Analysis"},{"location":"workflows/rnaseq/#pre-requisites","text":"Before starting the analysis, ensure you have the following: Aligned reads in SAM/BAM/CRAM format A reference annotation file in GTF or GFF format A read quantification tool such as featureCounts or HTSeq This document describes two pipelines for RNA-seq analysis: Using featureCounts and then analyzing with DESeq2 in R Using the Tuxedo Suite (HISAT2, StringTie, Ballgown)","title":"Pre-requisites"},{"location":"workflows/rnaseq/#featurecounts","text":"featureCounts is a program for counting reads mapped to genomic features, such as genes, exons, and promoters. It is part of the Subread package and can be used for RNA-seq as well as DNA-seq analysis. featureCounts takes as input SAM/BAM files and an annotation file including chromosomal coordinates of features. It outputs numbers of reads assigned to features (or meta-features). It also outputs stat info for the overall summrization results, including number of successfully assigned reads and number of reads that failed to be assigned due to various reasons (these reasons are included in the stat info). Use FeatureCounts to count the number of reads that map to each gene in a GTF file and summarize the results for downstream analysis (i.e., differential expression).","title":"featureCounts"},{"location":"workflows/rnaseq/#input","text":"Aligned reads in BAM format and a GTF file containing genomic features. Example usage: featureCounts -T \"$NUM_THREADS\" --verbose -t exon -g gene_id --countReadPairs \\ -a \"$REF_GTF\" -p -P -C -B -o \"${OUT_DIR}/${OUT_PREFIX}.tsv\" ./*.bam Or, more simply: featureCounts -a ${ANNOTATION} -p --countReadPairs -T$(nproc) -o total_counts.txt ./*.bam Annotation file from seqs_for_alignment_pipelines.ucsc_ids/ GCA_000001405.15_GRCh38_full_analysis_set.refseq_annotation.gff.gz 2024-09-10 13:15 74M GCA_000001405.15_GRCh38_full_analysis_set.refseq_annotation.gtf.gz","title":"Input"},{"location":"workflows/rnaseq/#output","text":"Feature counts are written to a tab-delimited file ( .tsv or .txt ) with columns for each sample. You can import this file into R or other statistical software for further analysis. Feature counts also provide a summary of the number of reads that were assigned to features, as well as the number of reads that were not assigned to any feature.","title":"Output"},{"location":"workflows/rnaseq/#downstream-analysis-using-r","text":"Export the results from featureCounts to R for further analysis, such as differential expression analysis using packages like DESeq2 or edgeR.","title":"Downstream Analysis using R"},{"location":"workflows/rnaseq/#normalization","text":"Caution Without between sample normalization NONE of these units are comparable across experiments. This is a result of RNA-Seq being a relative measurement, not an absolute one. Source . Counts usually refers to the number of reads that align to a particular feature ($X_i). These numbers are heavily dependent on two things: The amount of fragments you sequenced (this is related to relative abundances). The length of the feature, or more appropriately, the effective length . Effective length refers to the number of possible start sites a feature could have generated a fragment of that particular length. Since counts are NOT scaled by the length of the feature, all units in this category are not comparable within a sample without adjusting for the feature length. This means you can\u2019t sum the counts over a set of features to get the expression of that set (e.g. you can\u2019t sum isoform counts to get gene counts).","title":"Normalization"},{"location":"workflows/rnaseq/#tpm-transcripts-per-million","text":"Transcripts per million (TPM) is a measurement of the proportion of transcripts in your pool of RNA.","title":"TPM (Transcripts Per Million)"},{"location":"workflows/rnaseq/#rpkmfpkm","text":"Reads per kilobase of exon per million reads mapped (RPKM), or the more generic FPKM (substitute reads with fragments) are essentially the same thing. Note FPKM is not $2 *$ RPKM if you have paired-end reads. FPKM $==$ RPKM if you have single-end reads, and saying RPKM when you have paired-end reads is incorrect.","title":"RPKM/FPKM"},{"location":"workflows/rnaseq/#code-example","text":"countToTpm <- function(counts, effLen) { rate <- log(counts) - log(effLen) denom <- log(sum(exp(rate))) exp(rate - denom + log(1e6)) } countToFpkm <- function(counts, effLen) { N <- sum(counts) exp( log(counts) + log(1e9) - log(effLen) - log(N) ) } fpkmToTpm <- function(fpkm) { exp(log(fpkm) - log(sum(fpkm)) + log(1e6)) } countToEffCounts <- function(counts, len, effLen) { counts * (len / effLen) }","title":"Code example"},{"location":"workflows/rnaseq/#r-packages","text":"","title":"R Packages"},{"location":"workflows/rnaseq/#deseq2","text":"DESeq2 is an R package for differential gene expression analysis based on the negative binomial distribution. DESeq2 provides methods to test for differential expression by use of negative binomial generalized linear models. The models use the raw counts as input and perform regularized log transformation and variance stabilizing transformation. The package also provides functions to visualize the data and results.","title":"DESeq2"},{"location":"workflows/rnaseq/#tuxedo-suite","text":"The Tuxedo Suite is a collection of tools for transcript-level expression analysis of RNA-seq experiments. Align reads to the reference genome and sort to BAM format (HISAT2) Assemble transcripts (StringTie) Prepare for differential expression analysis (Ballgown setup) Perform differential expression analysis Visualize results Tool Description Manual Source HISAT2 Align reads to reference genome manual source StringTie Assemble RNA-Seq alignments into transcripts manual source Ballgown Isoform-level differential expression analysis manual source Tip StringTie comes packaged with gffcompare for comparing and evaluating the accuracy of RNA-seq transcript assemblers. Read the manual for details. Using Ballgown: Filter to remove low-abundance genes Identify differentially expressed transcripts and genes Add gene names Sort by p-value Write results to file","title":"Tuxedo Suite"},{"location":"workflows/rnaseq/#references","text":"Liao Y, Smyth GK, Shi W. featureCounts: an efficient general purpose program for assigning sequence reads to genomic features. Bioinformatics. 2014;30(7):923-30. PMID: 24227677 Pertea, M., Kim, D., Pertea, G. M., Leek, J. T., & Salzberg, S. L. (2016). Transcript-level expression analysis of RNA-seq experiments with HISAT, StringTie and Ballgown. Nature Protocols, 11(9), 1650\u20131667. PMID: 27560171 Kim D, Langmead B, Salzberg SL. HISAT: a fast spliced aligner with low memory requirements. Nat Methods. 2015;12(4):357-360. PMID: 25751142 Pertea M, Pertea GM, Antonescu CM, Chang TC, Mendell JT, Salzberg SL. StringTie enables improved reconstruction of a transcriptome from RNA-seq reads. Nat Biotechnol. 2015;33(3):290-295. PMID: 25690850 Frazee AC, Pertea G, Jaffe AE, Langmead B, Salzberg SL, Leek JT. Ballgown bridges the gap between transcriptome assembly and expression analysis. Nat Biotechnol. 2015;33(3):243-246. PMID: 25748911","title":"References"}]}